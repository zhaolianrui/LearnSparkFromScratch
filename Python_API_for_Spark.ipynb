{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>map vs mapPartitions</h2>\n",
    "<ul>\n",
    "<li><strong>map</strong> will not change the number of elements in an RDD, while <strong>mapPartitions</strong> might very well do so.</li>\n",
    "<li>The method <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=rdd#pyspark.RDD.map\">map</a> Return a new distributed dataset formed by passing each <em>element</em> of the source through a function func. <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=rdd#pyspark.RDD.mapPartitions\">mapPartitions</a> Similar to map, but runs separately on each <em>partition(block)</em> of the RDD, so <i>func</i> must be of type Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T.</li>\n",
    "<li><strong>map</strong> works the function being utilized at a per element level while  <strong>mapPartitions</strong> exercises the function at the partition level</li>\n",
    "</ul>\n",
    "<p><strong><em>Example Scenario</em></strong>: if we have 100K elements in a particular RDD partition then we will fire off the function being used by the mapping transformation 100K times when we use <strong>map</strong>.Conversely, if we use <strong>mapPartitions</strong> then we will only call the particular function one time, but we will pass in all 100K records and get back all responses in one function call.There will be performance gain since <strong>map</strong> works on a particular function so many times, especially if the function is doing something expensive each time that it wouldn't need to do if we passed in all the elements at once(in case of <strong>mapPartitions</strong>).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, None, 3, 4, 5]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5],2)\n",
    "rdd.map(lambda a:a if a!=2 else None).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_out_2(partition):\n",
    "    print(\"partition:\")\n",
    "    for element in partition:\n",
    "        print(element)\n",
    "        if element != 2:\n",
    "            yield element\n",
    "rdd.mapPartitions(filter_out_2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>shell打印输出:</i><br/>partition:\n",
    "3\n",
    "4\n",
    "5\n",
    "partition:\n",
    "1\n",
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>mapValues(f)</h2>\n",
    "<p>Pass each value in the key-value pair RDD through a map function without changing the keys; this also retains the original RDD’s partitioning.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
    "def f(x): return len(x)\n",
    "rdd.mapValues(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>groupByKey</h2>\n",
    "<p>When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs. </p>\n",
    "<p><b>Note:</b> If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using <em>reduceByKey</em> or <em>aggregateByKey</em> will yield much better performance.\n",
    "<br/>\n",
    "<b>Note:</b> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD.You can pass an optional <em>numTasks</em> argument to set a different number of tasks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1), ('a', 2)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "rdd.groupByKey().mapValues(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', [1]), ('a', [1, 1])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to use <b>reduceByKey</b> it will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1), ('a', 2)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>reduceByKey</h2>\n",
    "<p> When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function <i>func</i>, which must be of type (V,V) =&gt; V. Like in <em>groupByKey</em>, the number of reduce tasks is configurable through an optional second argument.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1), ('a', 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "rdd.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>aggregate(zeroValue, seqOp, combOp)</h2>\n",
    "<p>Aggregate the elements of each partition, and then the results for all the partitions, using a given combine functions and a neutral “zero value.”\n",
    "\n",
    "The functions op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object allocation; however, it should not modify t2.\n",
    "\n",
    "The first function (seqOp) can return a different result type, U, than the type of this RDD. Thus, we need one operation for merging a T into an U and one operation for merging two U.</p>\n",
    "<p>文档中给出了一个Example如下：</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为便于理解，代码修改如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seqOp(x,y):\n",
    "    if x ==(0,0):\n",
    "        print(\"partition-----\")\n",
    "    print(\"seq_x:\",x)\n",
    "    print(\"seq_y:\",y)\n",
    "    z = (x[0] + y,x[1] + 1)\n",
    "    print(\"seq_z:\",z)\n",
    "    return z\n",
    "def combOp(x,y):\n",
    "    print(\"comb_x\",x)\n",
    "    print(\"comb_y\",y)\n",
    "    z = (x[0] + y[0],x[1] + y[1])\n",
    "    print(\"comb_z:\",z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comb_x (0, 0)\n",
      "comb_y (3, 2)\n",
      "comb_z: (3, 2)\n",
      "comb_x (3, 2)\n",
      "comb_y (7, 2)\n",
      "comb_z: (10, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>shell打印输出:</i><br/>\n",
    "partition-----<br/>\n",
    "seq_x: (0, 0)<br/>\n",
    "seq_y: 3<br/>\n",
    "seq_z: (3, 1)<br/>\n",
    "seq_x: (3, 1)<br/>\n",
    "seq_y: 4<br/>\n",
    "seq_z: (7, 2)<br/>\n",
    "partition-----<br/>\n",
    "seq_x: (0, 0)<br/>\n",
    "seq_y: 1<br/>\n",
    "seq_z: (1, 1)<br/>\n",
    "seq_x: (1, 1)<br/>\n",
    "seq_y: 2<br/>\n",
    "seq_z: (3, 2)<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>从上面的代码的输出结果可以看出，1,2被分到第1个分区中，3,4被分到第2个分区中。在第1个分区中首先将zeroValue(0,0)和第一个元素3传给seqOp函数，返回(3,1)，然后将(3,1)和第二个元素4传给seqOp函数，返回(7,2)，以此类推，在第2个分区中还是先将zeroValue(0,0)和第一个元素1传给seqOp函数，返回(1,1)，然后将(1,1)和第二个元素1传给seqOp函数，返回(3,2)。最后将初始值zeroValue(0,0)和两个分区的结果经过combOp函数进行计算，先将初始值zeroValue(0,0)和第二个分区的结果(3，2)传给combine函数，返回(3，2)，然后将(3，2)和第一个分区结果(7,2)传给combine函数，返回最终结果(10, 4)。</p><p>因此对于<strong>aggregate</strong>总结如下：<br/>\n",
    "def aggregate[U](zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): U<br/>\n",
    "aggregate用户聚合RDD中的元素，先使用seqOp将RDD中每个分区中的T类型元素聚合成U类型，再使用combOp将之前每个分区聚合后的U类型聚合成U类型，特别注意seqOp和combOp都会使用zeroValue的值，zeroValue的类型为U。<br/>将初始值和第一个分区中的第一个元素传递给seq函数进行计算，然后将计算结果和第二个元素传递给seq函数，直到计算到最后一个值。第二个分区中也是同理操作。最后将所有分区的结果经过combine函数进行计算（先将前两个结果进行计算，将返回结果和下一个结果传给combine函数，以此类推），并返回最终结果。</p>\n",
    "<p>再举个例子如下：</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comOp3\t1\n",
      "comOp4\t3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize((1,2,3,4,5,6),2)\n",
    "def seq(a,b):\n",
    "    print ('seqOp:'+str(a)+\"\\t\"+str(b))\n",
    "    return min(a,b)\n",
    "def comb(a,b):\n",
    "    print ('comOp'+str(a)+\"\\t\"+str(b))\n",
    "    return a+b\n",
    "rdd.aggregate(3,seq,comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shell打印输出:<br/>\n",
    "seqOp:3\t1<br/>\n",
    "seqOp:1\t2<br/>\n",
    "seqOp:1\t3<br/>\n",
    "seqOp:3\t4<br/>\n",
    "seqOp:3\t5<br/>\n",
    "seqOp:3\t6<br/>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
