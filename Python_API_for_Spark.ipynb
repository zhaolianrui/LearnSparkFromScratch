{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>map</strong>(f, preservesPartitioning=False)<br/>\n",
    "<strong>mapPartitions</strong>(f, preservesPartitioning=False)<br/>\n",
    "<ul>\n",
    "<li><strong>map</strong> will not change the number of elements in an RDD, while <strong>mapPartitions</strong> might very well do so.</li>\n",
    "<li>The method <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=rdd#pyspark.RDD.map\">map</a> Return a new distributed dataset formed by passing each <em>element</em> of the source through a function func. <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=rdd#pyspark.RDD.mapPartitions\">mapPartitions</a> Similar to map, but runs separately on each <em>partition(block)</em> of the RDD, so <i>func</i> must be of type Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T.</li>\n",
    "<li><strong>map</strong> works the function being utilized at a per element level while  <strong>mapPartitions</strong> exercises the function at the partition level</li>\n",
    "</ul>\n",
    "<p><strong><em>Example Scenario</em></strong>: if we have 100K elements in a particular RDD partition then we will fire off the function being used by the mapping transformation 100K times when we use <strong>map</strong>.Conversely, if we use <strong>mapPartitions</strong> then we will only call the particular function one time, but we will pass in all 100K records and get back all responses in one function call.There will be performance gain since <strong>map</strong> works on a particular function so many times, especially if the function is doing something expensive each time that it wouldn't need to do if we passed in all the elements at once(in case of <strong>mapPartitions</strong>).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, None, 3, 4, 5]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5],2)\n",
    "rdd.map(lambda a:a if a!=2 else None).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_out_2(partition):\n",
    "    print(\"partition:\")\n",
    "    for element in partition:\n",
    "        print(element)\n",
    "        if element != 2:\n",
    "            yield element\n",
    "rdd.mapPartitions(filter_out_2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>shell打印输出:</i><br/>partition:\n",
    "3\n",
    "4\n",
    "5\n",
    "partition:\n",
    "1\n",
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>mapValues</strong>(f)<br/>\n",
    "&emsp;&emsp;Pass each value in the key-value pair RDD through a map function without changing the keys; this also retains the original RDD’s partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
    "def f(x): return len(x)\n",
    "rdd.mapValues(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>groupByKey</strong>(numPartitions=None, partitionFunc)<br/>\n",
    "&emsp;&emsp;When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs. \n",
    "<p><b>Note:</b> If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using <em>reduceByKey</em> or <em>aggregateByKey</em> will yield much better performance.\n",
    "<br/>\n",
    "<b>Note:</b> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD.You can pass an optional <em>numTasks</em> argument to set a different number of tasks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1), ('a', 2)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "rdd.groupByKey().mapValues(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', [1]), ('a', [1, 1])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to use <b>reduceByKey</b> it will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1), ('a', 2)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>reduceByKey</strong>(func, numPartitions=None, partitionFunc)<br/>\n",
    "&emsp;&emsp;When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function <i>func</i>, which must be of type (V,V) =&gt; V. Like in <em>groupByKey</em>, the number of reduce tasks is configurable through an optional second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1), ('a', 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "rdd.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>aggregate</strong>(zeroValue, seqOp, combOp)<br/>\n",
    "&emsp;&emsp;Aggregate the elements of each partition, and then the results for all the partitions, using a given combine functions and a neutral “zero value.”<br/>\n",
    "&emsp;&emsp;The functions op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object allocation; however, it should not modify t2.<br/>\n",
    "&emsp;&emsp;The first function (seqOp) can return a different result type, U, than the type of this RDD. Thus, we need one operation for merging a T into an U and one operation for merging two U.\n",
    "<p>&emsp;&emsp;文档中给出了一个Example如下：</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为便于理解，代码修改如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seqOp(x,y):\n",
    "    if x ==(0,0):\n",
    "        print(\"partition-----\")\n",
    "    print(\"seq_x:\",x)\n",
    "    print(\"seq_y:\",y)\n",
    "    z = (x[0] + y,x[1] + 1)\n",
    "    print(\"seq_z:\",z)\n",
    "    return z\n",
    "def combOp(x,y):\n",
    "    print(\"comb_x\",x)\n",
    "    print(\"comb_y\",y)\n",
    "    z = (x[0] + y[0],x[1] + y[1])\n",
    "    print(\"comb_z:\",z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comb_x (0, 0)\n",
      "comb_y (3, 2)\n",
      "comb_z: (3, 2)\n",
      "comb_x (3, 2)\n",
      "comb_y (7, 2)\n",
      "comb_z: (10, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>shell打印输出:</i><br/>\n",
    "partition-----<br/>\n",
    "seq_x: (0, 0)<br/>\n",
    "seq_y: 3<br/>\n",
    "seq_z: (3, 1)<br/>\n",
    "seq_x: (3, 1)<br/>\n",
    "seq_y: 4<br/>\n",
    "seq_z: (7, 2)<br/>\n",
    "partition-----<br/>\n",
    "seq_x: (0, 0)<br/>\n",
    "seq_y: 1<br/>\n",
    "seq_z: (1, 1)<br/>\n",
    "seq_x: (1, 1)<br/>\n",
    "seq_y: 2<br/>\n",
    "seq_z: (3, 2)<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>&emsp;&emsp;从上面的代码的输出结果可以看出，1,2被分到第1个分区中，3,4被分到第2个分区中。在第1个分区中首先将zeroValue(0,0)和第一个元素3传给seqOp函数，返回(3,1)，然后将(3,1)和第二个元素4传给seqOp函数，返回(7,2)，以此类推，在第2个分区中还是先将zeroValue(0,0)和第一个元素1传给seqOp函数，返回(1,1)，然后将(1,1)和第二个元素1传给seqOp函数，返回(3,2)。最后将初始值zeroValue(0,0)和两个分区的结果经过combOp函数进行计算，先将初始值zeroValue(0,0)和第二个分区的结果(3，2)传给combine函数，返回(3，2)，然后将(3，2)和第一个分区结果(7,2)传给combine函数，返回最终结果(10, 4)。</p><p>&emsp;&emsp;因此对于<strong>aggregate</strong>总结如下：<br/>\n",
    "&emsp;&emsp;def aggregate[U](zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): U<br/>\n",
    "&emsp;&emsp;aggregate用户聚合RDD中的元素，先使用seqOp将RDD中每个分区中的T类型元素聚合成U类型，再使用combOp将之前每个分区聚合后的U类型聚合成U类型，特别注意seqOp和combOp都会使用zeroValue的值，zeroValue的类型为U。<br/>&emsp;&emsp;将初始值和第一个分区中的第一个元素传递给seq函数进行计算，然后将计算结果和第二个元素传递给seq函数，直到计算到最后一个值。第二个分区中也是同理操作。最后将所有分区的结果经过combine函数进行计算（先将前两个结果进行计算，将返回结果和下一个结果传给combine函数，以此类推），并返回最终结果。</p>\n",
    "<p>&emsp;&emsp;再举个例子如下：</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comOp3\t1\n",
      "comOp4\t3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize((1,2,3,4,5,6),2)\n",
    "def seq(a,b):\n",
    "    print ('seqOp:'+str(a)+\"\\t\"+str(b))\n",
    "    return min(a,b)\n",
    "def comb(a,b):\n",
    "    print ('comOp'+str(a)+\"\\t\"+str(b))\n",
    "    return a+b\n",
    "rdd.aggregate(3,seq,comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shell打印输出:<br/>\n",
    "seqOp:3\t1<br/>\n",
    "seqOp:1\t2<br/>\n",
    "seqOp:1\t3<br/>\n",
    "seqOp:3\t4<br/>\n",
    "seqOp:3\t5<br/>\n",
    "seqOp:3\t6<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>aggregateByKey</strong>(zeroValue, seqFunc, combFunc, numPartitions, partitionFunc)<br/>\n",
    "&emsp;&emsp;Aggregate the values of each key, using given combine functions and a neutral “zero value”. This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U’s, The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U.<br/>&emsp;&emsp;该函数是对PairRDD中相同Key的值进行聚合操作，在聚合过程中同样使用了一个中立的初始值。和aggregate函数类似，aggregateByKey返回值的类型不需要和RDD中value的类型一致。因为aggregateByKey是对相同Key中的值进行聚合操作，所以aggregateByKey函数最终返回的类型还是PairRDD，对应的结果是Key和聚合好的值；而aggregate函数直接是返回非RDD的结果，这点需要注意。在实现过程中，定义了三个aggregateByKey函数原型，但最终调用的aggregateByKey函数都一致。\n",
    "<ol>\n",
    "<li>def aggregateByKey[U:ClassTag]\\(zeroValue:U, partitioner:Partitioner\\)<br/>(seqOp:(U, V) => U, combOp:(U, U) => U):RDD[(K, U)]</li>\n",
    "<li>def aggregateByKey[U:ClassTag]\\(zeroValue:U, numPartitions:Int\\)<br/>(seqOp:(U, V) => U, combOp:(U, U) => U):RDD[(K, U)]</li>\n",
    "<li>def aggregateByKey[U:ClassTag]\\(zeroValue:U\\)<br/>(seqOp:(U, V) => U, combOp:(U, U) => U):RDD[(K, U)]</li>\n",
    "</ol>\n",
    "<br/>&emsp;&emsp;第一个aggregateByKey函数我们可以自定义Partitioner。除了这个参数之外，其函数声明和aggregate很类似；其他的aggregateByKey函数实现最终都是调用这个。<br/>\n",
    "&emsp;&emsp;第二个aggregateByKey函数可以设置分区的个数(numPartitions)，最终用的是HashPartitioner。<br/>\n",
    "&emsp;&emsp;第三个aggregateByKey实现先会判断当前RDD是否定义了分区函数，如果定义了则用当前RDD的分区；如果当前RDD并未定义分区 ，则使用HashPartitioner。<br/><br/>\n",
    "&emsp;&emsp;这个函数中:<br/>\n",
    "&emsp;&emsp;U: ClassTag==>表示这个最终的RDD的返回值类型.<br/>\n",
    "&emsp;&emsp;zeroValue: U==>表示在每个分区中第一次拿到key值时,用于创建一个返回类型的函数,这个函数最终会被包装成先生成一个返回类型,然后通过调用seqOp函数,把第一个key对应的value添加到这个类型U的变量中,下面代码的红色部分.<br/>\n",
    "&emsp;&emsp;seqOp: (U,V) => U ==> 这个用于把迭代分区中key对应的值添加到zeroValue创建的U类型实例中.<br/>\n",
    "&emsp;&emsp;combOp: (U,U) => U ==> 这个用于合并每个分区中聚合过来的两个U类型的。</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 3), (1, 7)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([(1,3),(1,2),(1,4),(2,3)])\n",
    "def seq(a,b):\n",
    "    if a == 1:\n",
    "        print(\"partition:\")\n",
    "    print(\"seq_a:\",a)\n",
    "    print(\"seq_b:\",b)\n",
    "    return max(a,b)\n",
    "def comb(a,b):\n",
    "    print(\"comb_a:\",a)\n",
    "    print(\"comb_b:\",b)\n",
    "    return a+b\n",
    "data.aggregateByKey(1,seq,comb).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>shell output:</em><br/>\n",
    "partition:<br/>\n",
    "seq_a: 1<br/>\n",
    "seq_b: 3<br/>\n",
    "seq_a: 3<br/>\n",
    "seq_b: 2<br/>\n",
    "partition:<br/>\n",
    "seq_a: 1<br/>\n",
    "seq_b: 4<br/>\n",
    "partition:<br/>\n",
    "seq_a: 1<br/>\n",
    "seq_b: 3<br/>\n",
    "comb_a: 3<br/>\n",
    "comb_b: 4<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>foreach</strong>(f)<br/>\n",
    "&emsp;&emsp;Applies a function to each partition of this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x): print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>shell output：</em><br/>\n",
    "1<br/>\n",
    "2<br/>\n",
    "3<br/>\n",
    "4<br/>\n",
    "5<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>groupBy</strong>(f, numPartitions, partitionFunc)<br/>\n",
    "&emsp;&emsp;Return an RDD of grouped items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "result = rdd.groupBy(lambda x: x % 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, <pyspark.resultiterable.ResultIterable at 0x7f8fec386400>),\n",
       " (1, <pyspark.resultiterable.ResultIterable at 0x7f8fec3867f0>)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 8]), (1, [1, 1, 3, 5])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x, sorted(y)) for (x, y) in result]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
