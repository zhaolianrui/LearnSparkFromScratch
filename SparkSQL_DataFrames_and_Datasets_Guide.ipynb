{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Spark SQL, DataFrames and Datasets Guide</h1>\n",
    "<ul>\n",
    "  <li>Overview<ul>\n",
    "      <li>SQL</li>\n",
    "      <li>Datasets and DataFrames</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Getting Started<ul>\n",
    "      <li>Starting Point: SparkSession</li>\n",
    "      <li>Creating DataFrames</li>\n",
    "      <li>Untyped Dataset Operations(aka DataFrame Operations)</li>\n",
    "      <li>Running SQL Queries Programmatically</li>\n",
    "      <li>Creating Datasets</li>\n",
    "      <li>Interoperating with RDDs<ul>\n",
    "          <li>Inferring the Schema Using Reflection</li>\n",
    "          <li>Programmatically Specifying the Schema</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Data Sources<ul>\n",
    "      <li>Generic Load/Save Functions<ul>\n",
    "          <li>Manually Specifying Options</li>\n",
    "          <li>Run SQL on files directly</li>\n",
    "          <li>Save Modes</li>\n",
    "          <li>Saving to Persistent Tables</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>Parquet Files<ul>\n",
    "          <li>Loading Data Programmatically</li>\n",
    "          <li>Partition Discovery</li>\n",
    "          <li>Schema Merging</li>\n",
    "          <li>Hive metastore Parquet table conversion<ul>\n",
    "              <li>Hive/Parquet Schema Reconciliation</li>\n",
    "              <li>Metadata Refreshing</li>\n",
    "            </ul>\n",
    "          </li>\n",
    "          <li>Configuration</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>JSON Datasets</li>\n",
    "      <li>Hive Tables<ul>\n",
    "          <li>Interacting with Different Versions of Hive Metastore</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>JDBC To Other Databases</li>\n",
    "      <li>Troubleshooting</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Performance Tuning<ul>\n",
    "      <li>Caching Data In Memory</li>\n",
    "      <li>Other Configuration Options</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Distributed SQL Engine<ul>\n",
    "      <li>Running the Thrift JDBC/ODBC server</li>\n",
    "      <li>Running the Spark SQL CLI</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Migration Guide<ul>\n",
    "      <li>Upgrading From Spark SQL 1.6 to 2.0</li>\n",
    "      <li>Upgrading From Spark SQL 1.5 to 1.6</li>\n",
    "      <li>Upgrading From Spark SQL 1.4 to 1.5</li>\n",
    "      <li>Upgrading from Spark SQL 1.3 to 1.4 <ul>\n",
    "          <li>DataFrame data reader/writer interface</li>\n",
    "          <li>DataFrame.groupBy retains grouping columns</li>\n",
    "          <li>Behavior change on DataFrame.withColumn</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>Upgrading from Spark SQL 1.0-1.2 to 1.3<ul>\n",
    "          <li>Rename of SchemaRDD to DataFrame</li>\n",
    "          <li>Unification of the Java and Scala APIs</li>\n",
    "          <li>Isolation of Implicit Conversions and Removal of dsl Package (Scala-only)</li>\n",
    "          <li>Removal of the type aliases in org.apache.spark.sql for DataType (Scala-only)</li>\n",
    "          <li>UDF Registration Moved to <code>sqlContext.udf</code> (Java &amp; Scala)</li>\n",
    "          <li>Python DataTypes No Longer Singletons</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>Compatibility with Apache Hive<ul>\n",
    "          <li>Deploying in Existing Hive Warehouses</li>\n",
    "          <li>Supported Hive Features</li>\n",
    "          <li>Unsupported Hive Functionality</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Reference    <ul>\n",
    "      <li>Data Types</li>\n",
    "      <li>NaN Semantics</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "<h1>Overview</h1>\n",
    "<p>Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided\n",
    "by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally,Spark SQL uses this extra information to perform extra optimizations. There are several ways to\n",
    "interact with Spark SQL including SQL and the Dataset API. When computing a result the same execution engine is used, independent of which API/language you are using to express the computation. This unification(统一，联合; 一致;\n",
    ") means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation.</p>\n",
    "<p>All of the examples on this page use sample data included in the Spark distribution and can be run in the <code>spark-shell</code>, <code>pyspark</code> shell, or <code>sparkR</code> shell.</p>\n",
    "\n",
    "<h2>SQL</h2>\n",
    "<p>One use of Spark SQL is to execute SQL queries.Spark SQL can also be used to read data from an existing Hive installation. For more on how to configure this feature, please refer to the <strong>Hive Tables</strong> section. When running SQL from within another programming language the results will be returned as a <strong>Dataset/DataFrame</strong>.You can also interact with the SQL interface using the <strong>command-line</strong> or over <strong>JDBC/ODBC</strong>.</p>\n",
    "\n",
    "<h2>Datasets and DataFrames</h2>\n",
    "\n",
    "<p>A Dataset is a distributed collection of data.Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL&#8217;s optimized execution engine. A Dataset can be <strong>constructed</strong> from JVM objects and then\n",
    "manipulated using functional transformations (<code>map</code>, <code>flatMap</code>, <code>filter</code>, etc.).\n",
    "The Dataset API is available in <a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\">Scala</a> and\n",
    "<a href=\"http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\">Java</a>. Python does not have the support for the Dataset API. But due to Python&#8217;s dynamic nature,many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally<code>row.columnName</code>). The case for R is similar.</p>\n",
    "\n",
    "<p>A DataFrame is a <em>Dataset</em> organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood(在幕后). DataFrames can be constructed from a wide array of <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources\">sources</a> such as: <u>structured data files, tables in Hive, external databases, or existing RDDs</u>.The DataFrame API is available in Scala,Java, <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\">Python</a>, and <a href=\"http://spark.apache.org/docs/latest/api/R/index.html\">R</a>.In Scala and Java, a DataFrame is represented by a Dataset of <code>Row</code>s.In <a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\">the Scala API</a>, <code>DataFrame</code> is simply a type alias of <code>Dataset[Row]</code>.While, in <a href=\"http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\">Java API</a>, users need to use <code>Dataset&lt;Row&gt;</code> to represent a <code>DataFrame</code>.</p>\n",
    "\n",
    "<p>Throughout this document, we will often refer to Scala/Java Datasets of <code>Row</code>s as DataFrames.</p>\n",
    "\n",
    "<h1>Getting Started</h1>\n",
    "<h2>Starting Point: SparkSession</h2>\n",
    "<p>The entry point into(入口点) all functionality in Spark is the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession\"><code>SparkSession</code></a> class. To create a basic <code>SparkSession</code>, just use <code>SparkSession.builder</code>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"PythonSQL\")\\\n",
    ".config(\"spark.some.config.option\", 'some-value')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><code>SparkSession</code> in Spark 2.0 provides builtin support for Hive features including the ability to write queries using HiveQL, access to Hive UDFs(User defined Function,用户自定义函数), and the ability to read data from Hive tables.To use these features, you do not need to have an existing Hive setup.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"creating-dataframes\">Creating DataFrames</h2>\n",
    "<p>With a <code>SparkSession</code>, applications can create DataFrames from an <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds\">existing <code>RDD</code></a>,from a Hive table, or from <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources\">Spark data sources</a>.</p>\n",
    "\n",
    "<p>As an example, the following creates a DataFrame based on the content of a JSON file:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "df = spark.read.json(\"resources/people.json\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Untyped Dataset Operations (aka DataFrame Operations)</h2>\n",
    "<p>DataFrames provide a domain-specific language(DSL:领域特定语言) for structured data manipulation in <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\">Scala</a>, <a href=\"https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\">Java</a>, <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\">Python</a> and <a href=\"https://spark.apache.org/docs/latest/api/R/DataFrame.htmll\">R</a>.</p>\n",
    "<p>As mentioned above, in Spark 2.0, DataFrames are just Dataset of <code>Row</code>s in Scala and Java API. These operations are also referred as “untyped transformations” in contrast to “typed transformations” come with strongly typed(强类型) Scala/Java Datasets.</p>\n",
    "<p>Here we include some basic examples of structured data processing using Datasets:</p>\n",
    "<p>In Python it’s possible to access a DataFrame’s columns either by attribute (<code>df.age</code>) or by indexing (<code>df['age']</code>). While the former(前者) is convenient for interactive data exploration, users are highly encouraged to use the latter form, which is future proof and won’t break with column names that are also attributes on the DataFrame class.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark, df are from the previous example\n",
    "# Print the schema in a tree format\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select everybody, but increment the age by 1\n",
    "df.select(df[\"name\"], df[\"age\"] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select people older than 21\n",
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count people by age\n",
    "df.groupBy('age').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For a complete list of the types of operations that can be performed on a DataFrame refer to the <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\">API Documentation</a>.</p>\n",
    "<p>In addition to simple column references and expressions, DataFrames also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">DataFrame Function Reference</a>.</p>\n",
    "<h2>Running SQL Queries Programmatically(编码的方式运行SQL查询)</h2>\n",
    "<ul>\n",
    "<li><b>Scala</b></li><li><b>Java</b></li>\n",
    "<li><b>Python</b></li><li><b>R</b></li>\n",
    "</ul>\n",
    "<p>The <code>sql</code> function on a <code>SparkSession</code> enables applications to run SQL queries programmatically and returns the result as a <code>DataFrame</code>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>\n",
    "<a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=createorreplacetempview#pyspark.sql.DataFrame.createOrReplaceTempView\" ><tt>createOrReplaceTempView</tt><big>(</big><em>name</em><big>)</big></a></dt>\n",
    "<dd><p>Creates or replaces a temporary view with this DataFrame.</p>\n",
    "<p>The lifetime of this temporary table is tied to the <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=createorreplacetempview#pyspark.sql.SparkSession\" target=\"_blank\"><tt>SparkSession</tt></a>\n",
    "that was used to create this <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=createorreplacetempview#pyspark.sql.DataFrame\"><tt>DataFrame</tt></a>.</p>\n",
    "</dd></dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "sqlDF = spark.sql(\"select * from people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating Datasets(仅支持Scala,Java)</h2>\n",
    "<p>Datasets are similar to RDDs, however, instead of using Java serialization or Kryo they use\n",
    "a specialized <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder\">Encoder</a> to serialize the objects\n",
    "for processing or transmitting over the network. While both encoders and standard serialization are\n",
    "responsible for turning an object into bytes, encoders are code generated dynamically and use a format\n",
    "that allows Spark to perform many operations like filtering, sorting and hashing without deserializing\n",
    "the bytes back into an object.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Interoperating with(互操作) RDDs</h2>\n",
    "\n",
    "<p>Spark SQL supports two different methods for converting existing RDDs into Datasets. The first method uses reflection(反射) to infer(推断; 猜想) the schema of an RDD that contains specific types of objects. This reflection based approach leads to more concise code(简洁的代码) and works well when you already know the schema while writing your Spark application.</p>\n",
    "<p>The second method for creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose(冗长的，啰唆的，累赘的;), it allows you to construct Datasets when the columns and their types are not known until runtime.</p>\n",
    "\n",
    "<h3>Inferring the Schema Using Reflection</h3>\n",
    "<p>Spark SQL can convert an RDD of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=row#pyspark.sql.Row\" target=\"_blank\">Row</a> objects to a DataFrame, inferring the datatypes. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys of this list define the column names of the table, and the types are inferred by sampling the whole datase, similar to the inference that is performed on JSON files.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael, 29', 'Andy, 30', 'Justin, 19']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"resources/people.txt\")\n",
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Michael', ' 29'], ['Andy', ' 30'], ['Justin', ' 19']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = lines.map(lambda line: line.split(\",\"))\n",
    "parts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=29, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = parts.map(lambda people: Row(name=people[0], age=int(people[1])))\n",
    "people.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Justin\n"
     ]
    }
   ],
   "source": [
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Programmatically Specifying the Schema</h3>\n",
    "<p>When case classes cannot be defined ahead of time (for example,the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users),a <code>DataFrame</code> can be created programmatically with three steps.</p>\n",
    "<ol>\n",
    "  <li>Create an RDD of <code>Row</code>s from the original RDD;</li>\n",
    "  <li>Create the schema represented by a <a  href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=structtype#pyspark.sql.types.StructType\" target=\"_blank\"><code>StructType</code></a> matching the structure of\n",
    "<code>Row</code>s in the RDD created in Step 1.</li>\n",
    "  <li>Apply the schema to the RDD of <code>Row</code>s via <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=createdataframe#pyspark.sql.SparkSession.createDataFrame\" target=\"_blank\"><code>createDataFrame</code></a> method provided by <code>SparkSession</code>.</li>\n",
    "</ol>\n",
    "<p>For example:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fields: [StructField(name,StringType,true), StructField(age,StringType,true)] \n",
      "\n",
      "schema: StructType(List(StructField(name,StringType,true),StructField(age,StringType,true))) \n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"resources/people.txt\")\n",
    "parts = lines.map(lambda line: line.split(\",\"))\n",
    "# Each line is converted to a tuple.\n",
    "people = parts.map(lambda people: (people[0], people[1].strip()))\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"name age\"\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "print(\"fields:\", fields, '\\n')\n",
    "schema = StructType([StructField(\"name\", StringType(), True), StructField(\"age\", StringType(), True)])\n",
    "print(\"schema:\", schema, '\\n')\n",
    "# Apply the schema to the RDD.\n",
    "schemaPeople = spark.createDataFrame(people, schema)\n",
    "# Creates a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT name FROM people\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Sources</h1>\n",
    "\n",
    "<p>Spark SQL supports operating on a variety of data sources through the DataFrame interface.A DataFrame can be operated on using relational transformations and can also be used to create a temporary view.<strong>Registering a DataFrame as a temporary view allows you to run SQL queries over its data</strong>. This section describes the general methods for loading and saving data using the Spark Data Sources and then goes into specific options that are available for the built-in data sources.</p>\n",
    "\n",
    "<h2>Generic Load/Save Functions</h2>\n",
    "\n",
    "<p>In the simplest form, the default data source(<code>parquet</code> <u>unless otherwise configured</u>(除非另外配置) by <code>spark.sql.sources.default</code>) will be used for all operations.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.load(\"resources/users.parquet\")\n",
    "df.select(\"name\",\"favorite_color\").write.save(\"resources/namesAndFavColors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Manually Specifying Options</h3>\n",
    "\n",
    "<p>You can also manually specify the data source that will be used along with any extra options that you would like to pass to the data source. Data sources are specified by their fully qualified name (i.e., <code>org.apache.spark.sql.parquet</code>), but for built-in sources you can also use their short names (<code>json</code>, <code>parquet</code>, <code>jdbc</code>). <strong>DataFrames loaded from any data source type can be converted into other types using this syntax</strong>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.load(\"resources/people.json\", format=\"json\")\n",
    "df.select(\"name\", \"age\").write.save(\"resources/namesAndAges.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Run SQL on files directly</h3>\n",
    "\n",
    "<p>Instead of using read API to load a file into DataFrame and query it, you can also query that file directly with SQL.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM parquet.`resources/users.parquet`\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save Modes</h3>\n",
    "\n",
    "<p>Save operations can optionally take a <code>SaveMode</code>, that specifies how to handle existing data if\n",
    "present. <font color=\"red\"><strong>It is important to realize that these save modes do not utilize(利用;运用;使用;应用) any locking and are not atomic. Additionally, when performing an <code>Overwrite</code>, the data will be deleted before writing out the new data</strong>.</font></p>\n",
    "\n",
    "<table>\n",
    "<tr><th>Scala/Java</th><th>Any Language</th><th>Meaning</th></tr>\n",
    "<tr>\n",
    "  <td><code>SaveMode.ErrorIfExists</code> (default)</td>\n",
    "  <td><code>\"error\"</code> (default)</td>\n",
    "  <td>\n",
    "    When saving a DataFrame to a data source, if data already exists,an exception is expected to be thrown.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><code>SaveMode.Append</code></td>\n",
    "  <td><code>\"append\"</code></td>\n",
    "  <td>\n",
    "    When saving a DataFrame to a data source, if data/table already exists,contents of the DataFrame are expected to be appended to existing data.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><code>SaveMode.Overwrite</code></td>\n",
    "  <td><code>\"overwrite\"</code></td>\n",
    "  <td>\n",
    "    Overwrite mode means that when saving a DataFrame to a data source,if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><code>SaveMode.Ignore</code></td>\n",
    "  <td><code>\"ignore\"</code></td>\n",
    "  <td>\n",
    "    Ignore mode means that when saving a DataFrame to a data source, if data already exists,the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a <code>CREATE TABLE IF NOT EXISTS</code> in SQL.\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<h3>Saving to Persistent Tables</h3>\n",
    "\n",
    "<p><code>DataFrames</code> can also be saved as persistent tables into Hive metastore using the <strong><code>saveAsTable</code></strong>\n",
    "command. Notice existing Hive deployment is not necessary to use this feature. Spark will create a default local Hive metastore (using Derby) for you. Unlike the <strong><code>createOrReplaceTempView</code></strong> command,<strong><code>saveAsTable</code></strong> will materialize(实现) the contents of the DataFrame and create a pointer to the data in the Hive metastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the <strong><code>table</code></strong> method on a <strong><code>SparkSession</code></strong> with the name of the table.</p>\n",
    "\n",
    "<p>By default <strong><code>saveAsTable</code></strong> will create a &#8220;managed table&#8221;, meaning that the location of the data will be controlled by the metastore. Managed tables will also have their data deleted automatically when a table is dropped.</p>\n",
    "<p>在Hive上有两种类型的表，一种是Managed Table，另一种是External Table。它俩的主要区别在于：当我们drop表时，Managed Table会同时删去data和meta data，而External Table只会删meta data。</p>\n",
    "<h2>Parquet Files</h2>\n",
    "\n",
    "<p><a href=\"http://parquet.io\">Parquet</a> is a columnar format that is supported by many other data processing systems.Spark SQL provides support for both reading and writing Parquet files that automatically preserves(保留；保护；保存；维护) the schema of the original data. When writing Parquet files, all columns are automatically converted to be nullable(允许空值) for compatibility reasons(出于兼容方面的考虑).</p>\n",
    "\n",
    "<h3>Loading Data Programmatically</h3>\n",
    "\n",
    "<p>Using the data from the above example:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF = spark.read.json(\"resources/people.json\")\n",
    "\n",
    "# DataFrames can be saved as Parquet files, maintaining the schema information.\n",
    "peopleDF.write.parquet(\"resources/people.parquet\")\n",
    "\n",
    "# Read in the Parquet file created above.\n",
    "# Parquet files are self-describing so the schema is preserved.\n",
    "# The result of loading a parquet file is also a DataFrame.\n",
    "parquetFile = spark.read.parquet(\"resources/people.parquet\")\n",
    "# Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "teenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
    "teenagers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Partition Discovery(解析分区信息)</h3>\n",
    "<p>Table partitioning(表分区) is a common optimization approach(优化方法) used in systems like Hive. In a partitioned table(分区表), data are usually stored in different directories, with partitioning column values encoded(编码) in the path of each partition directory. The Parquet data source is now able to discover and infer partitioning information automatically. For example, we can store all our previously used population data into a partitioned table using the following directory structure, with two extra columns, <code>gender</code>(性别) and <code>country</code>(国家) as partitioning columns:</p>\n",
    "<p>类似hive的分区表，在分区表中数据会分开存储在不同的文件夹，区分的标准是分区字段，现在parquet的数据源可以自动的发现分区信息.</p>\n",
    "<pre><code>path\n",
    "└── to\n",
    "    └── table\n",
    "        ├── gender=male\n",
    "        │   ├── ...\n",
    "        │   │\n",
    "        │   ├── country=US\n",
    "        │   │   └── data.parquet\n",
    "        │   ├── country=CN\n",
    "        │   │   └── data.parquet\n",
    "        │   └── ...\n",
    "        └── gender=female\n",
    "            ├── ...\n",
    "            │\n",
    "            ├── country=US\n",
    "            │   └── data.parquet\n",
    "            ├── country=CN\n",
    "            │   └── data.parquet\n",
    "            └── ...</code></pre>\n",
    "<p>By passing <code>path/to/table</code> to either <code>SparkSession.read.parquet</code> or <code>SparkSession.read.load</code>, Spark SQL will automatically extract the partitioning information from the paths.Now the schema of the returned DataFrame becomes:</p>\n",
    "<p>通过传递path/to/table给SparkSession.read.parquet或SparkSession.read.load，Spark SQL将自动解析分区信息。返回的DataFrame的Schema如下：</p>\n",
    "<pre><code>root\n",
    "|-- name: string (nullable = true)\n",
    "|-- age: long (nullable = true)\n",
    "|-- gender: string (nullable = true)\n",
    "|-- country: string (nullable = true)</code></pre>\n",
    "<p>Notice that the data types of the partitioning columns are automatically inferred. Currently, numeric data types and string type are supported. Sometimes users may not want to automatically infer the data types of the partitioning columns. For these use cases, the automatic type inference can be configured by <code>spark.sql.sources.partitionColumnTypeInference.enabled</code>, which is default to <code>true</code>. When type inference is disabled, string type will be used for the partitioning columns.</p>\n",
    "<p>需要注意的是，数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：spark.sql.sources.partitionColumnTypeInference.enabled，默认值为true。如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。</p>\n",
    "<p>Starting from Spark 1.6.0, partition discovery only finds partitions under the given paths by default. For the above example, if users pass <code>path/to/table/gender=male</code> to either <code>SparkSession.read.parquet</code> or <code>SparkSession.read.load</code>, <code>gender</code> will not be considered as a partitioning column. If users need to specify the base path that partition discovery should start with, they can set <code>basePath</code> in the data source options. For example, when <code>path/to/table/gender=male</code> is the path of the data and users set <code>basePath</code> to <code>path/to/table/</code>, <code>gender</code> will be a partitioning column.</p>\n",
    "<p>从1.6版本开始，如果用了分区路径，那么会丢失用于分区路径的那个字段，只能通过basedir拿到，而把分区属性变成分区字段.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"resources/people.parquet\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=partitionby#pyspark.sql.DataFrameWriter.partitionBy\" target=\"_blank\">\n",
    "<dt>\n",
    "<tt>partitionBy</tt><big>(</big><em>*cols</em><big>)</big></dt></a>\n",
    "<dd><p>Partitions the output by the given columns on the file system.</p>\n",
    "<p>If specified, the output is laid out on the file system similar\n",
    "to Hive’s partitioning scheme.</p>\n",
    "<table rules=\"none\">\n",
    "<colgroup><col class=\"field-name\">\n",
    "<col class=\"field-body\">\n",
    "</colgroup><tbody valign=\"top\">\n",
    "<tr class=\"field-odd field\"><th class=\"field-name\">Parameters:</th><td class=\"field-body\"><strong>cols</strong> – name of columns</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<div class=\"highlight-python\"><div class=\"highlight\"><pre><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"o\">.</span><span class=\"n\"><span class=\"highlighted\">partitionBy</span></span><span class=\"p\">(</span><span class=\"s\">'year'</span><span class=\"p\">,</span> <span class=\"s\">'month'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">parquet</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">tempfile</span><span class=\"o\">.</span><span class=\"n\">mkdtemp</span><span class=\"p\">(),</span> <span class=\"s\">'data'</span><span class=\"p\">))</span>\n",
    "</pre></div>\n",
    "</div>\n",
    "</dd></dl>\n",
    "<p>从这个函数我们可以看出按照我们给出的字段完成了一个partition的过程，接受一个多参数的字段列表.</p>\n",
    "<font color=\"red\">关于Partition Discovery更多详情请参考<a href=\"http://blog.csdn.net/cjuexuan/article/details/51165310\" target=\"_blank\">这篇博文</a>。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.read.json(\"resources/people.json\").write.partitionBy(\"name\").parquet(\"resources/peoplePartitionBy.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>查看输出路径下的目录：</h3>\n",
    "<pre><code >_SUCCESS      name=Michael        name=Justin        name=Andy</code></pre>\n",
    "\n",
    "<p>我们可以发现按照我们的分区规则，在输出路径上建立了三个文件夹，分别为name=Michael,name=Justin,name=Andy</p>\n",
    "<h3>读取：</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 30|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"resources/peoplePartitionBy.parquet/name=Andy\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"resources/peoplePartitionBy.parquet/name=Michael\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"resources/peoplePartitionBy.parquet/name=Justin\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"resources/peoplePartitionBy.parquet\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>\n",
    "<tt>filter</tt><big>(</big><em>condition</em><big>)</big><a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=filter#pyspark.sql.DataFrame.filter\" target=\"_blank\">¶</a></dt>\n",
    "<dd><p>Filters rows using the given condition.</p>\n",
    "<p><a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=filter#pyspark.sql.DataFrame.where\"><tt>where()</tt></a> is an alias for <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=filter#pyspark.sql.DataFrame.filter\"><tt>filter()</tt></a>.</p>\n",
    "<table frame=\"void\" rules=\"none\">\n",
    "<colgroup><col>\n",
    "<col>\n",
    "</colgroup><tbody valign=\"top\">\n",
    "<tr><th>Parameters:</th><td><strong>condition</strong> – a <a  href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=filter#pyspark.sql.Column\" ><tt>Column</tt></a> of <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=filter#pyspark.sql.types.BooleanType\" ><tt>types.BooleanType</tt></a>\n",
    "or a string of SQL expression.</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<pre>&gt;&gt;&gt; df.filter(.age&gt;3).collect()\n",
    "[Row(age=5, name=u'Bob')]\n",
    "&gt;&gt;&gt; df.where(df.age==2).collect()\n",
    "[Row(age=2, name=u'Alice')]\n",
    "</pre>\n",
    "<pre>&gt;&gt;&gt; df.filter(\"age &gt; 3\").collect()\n",
    "[Row(age=5, name=u'Bob')]\n",
    "&gt;&gt;&gt; df.where(\"age = 2\").collect()\n",
    "[Row(age=2, name=u'Alice')]\n",
    "</pre>\n",
    "</dd></dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=30, name='Andy')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet(\"resources/peoplePartitionBy.parquet\")\n",
    "df.filter(df.name == \"Andy\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"resources/peoplePartitionBy.parquet\").filter('name=\"Andy\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Schema Merging</h3>\n",
    "<p>Like ProtocolBuffer, Avro, and Thrift, Parquet also supports schema evolution(演变; 进化; 发展;). Users can start with a simple schema, and gradually add more columns to the schema as needed. In this way, users may end up with multiple Parquet files with different but mutually compatible(相互兼容) schemas. The Parquet data source is now able to automatically detect this case and merge schemas of all these files.</p>\n",
    "\n",
    "<p>Since schema merging is a relatively expensive operation, and is not a necessity in most cases, we turned it off by default starting from 1.5.0. You may enable it by</p>\n",
    "<ol>\n",
    "  <li>setting data source option <code>mergeSchema</code> to <code>true</code> when reading Parquet files (as shown in the examples below), or</li>\n",
    "  <li>setting the global SQL option <code>spark.sql.parquet.mergeSchema</code> to <code>true</code>.</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
