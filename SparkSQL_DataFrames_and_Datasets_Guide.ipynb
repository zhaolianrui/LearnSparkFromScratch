{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Spark SQL, DataFrames and Datasets Guide</h1>\n",
    "<ul>\n",
    "  <li>Overview<ul>\n",
    "      <li>SQL</li>\n",
    "      <li>Datasets and DataFrames</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Getting Started<ul>\n",
    "      <li>Starting Point: SparkSession</li>\n",
    "      <li>Creating DataFrames</li>\n",
    "      <li>Untyped Dataset Operations(aka DataFrame Operations)</li>\n",
    "      <li>Running SQL Queries Programmatically</li>\n",
    "      <li>Creating Datasets</li>\n",
    "      <li>Interoperating with RDDs<ul>\n",
    "          <li>Inferring the Schema Using Reflection</li>\n",
    "          <li>Programmatically Specifying the Schema</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Data Sources<ul>\n",
    "      <li>Generic Load/Save Functions<ul>\n",
    "          <li>Manually Specifying Options</li>\n",
    "          <li>Run SQL on files directly</li>\n",
    "          <li>Save Modes</li>\n",
    "          <li>Saving to Persistent Tables</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>Parquet Files<ul>\n",
    "          <li>Loading Data Programmatically</li>\n",
    "          <li>Partition Discovery</li>\n",
    "          <li>Schema Merging</li>\n",
    "          <li>Hive metastore Parquet table conversion<ul>\n",
    "              <li>Hive/Parquet Schema Reconciliation</li>\n",
    "              <li>Metadata Refreshing</li>\n",
    "            </ul>\n",
    "          </li>\n",
    "          <li>Configuration</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>JSON Datasets</li>\n",
    "      <li>Hive Tables<ul>\n",
    "          <li>Interacting with Different Versions of Hive Metastore</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>JDBC To Other Databases</li>\n",
    "      <li>Troubleshooting</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Performance Tuning<ul>\n",
    "      <li>Caching Data In Memory</li>\n",
    "      <li>Other Configuration Options</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Distributed SQL Engine<ul>\n",
    "      <li>Running the Thrift JDBC/ODBC server</li>\n",
    "      <li>Running the Spark SQL CLI</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Migration Guide<ul>\n",
    "      <li>Upgrading From Spark SQL 1.6 to 2.0</li>\n",
    "      <li>Upgrading From Spark SQL 1.5 to 1.6</li>\n",
    "      <li>Upgrading From Spark SQL 1.4 to 1.5</li>\n",
    "      <li>Upgrading from Spark SQL 1.3 to 1.4 <ul>\n",
    "          <li>DataFrame data reader/writer interface</li>\n",
    "          <li>DataFrame.groupBy retains grouping columns</li>\n",
    "          <li>Behavior change on DataFrame.withColumn</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>Upgrading from Spark SQL 1.0-1.2 to 1.3<ul>\n",
    "          <li>Rename of SchemaRDD to DataFrame</li>\n",
    "          <li>Unification of the Java and Scala APIs</li>\n",
    "          <li>Isolation of Implicit Conversions and Removal of dsl Package (Scala-only)</li>\n",
    "          <li>Removal of the type aliases in org.apache.spark.sql for DataType (Scala-only)</li>\n",
    "          <li>UDF Registration Moved to <code>sqlContext.udf</code> (Java &amp; Scala)</li>\n",
    "          <li>Python DataTypes No Longer Singletons</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>Compatibility with Apache Hive<ul>\n",
    "          <li>Deploying in Existing Hive Warehouses</li>\n",
    "          <li>Supported Hive Features</li>\n",
    "          <li>Unsupported Hive Functionality</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Reference    <ul>\n",
    "      <li>Data Types</li>\n",
    "      <li>NaN Semantics</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "<h1>Overview</h1>\n",
    "<p>Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided\n",
    "by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally,Spark SQL uses this extra information to perform extra optimizations. There are several ways to\n",
    "interact with Spark SQL including SQL and the Dataset API. When computing a result the same execution engine is used, independent of which API/language you are using to express the computation. This unification(统一，联合; 一致;\n",
    ") means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation.</p>\n",
    "<p>All of the examples on this page use sample data included in the Spark distribution and can be run in the <code>spark-shell</code>, <code>pyspark</code> shell, or <code>sparkR</code> shell.</p>\n",
    "\n",
    "<h2>SQL</h2>\n",
    "<p>One use of Spark SQL is to execute SQL queries.Spark SQL can also be used to read data from an existing Hive installation. For more on how to configure this feature, please refer to the <strong>Hive Tables</strong> section. When running SQL from within another programming language the results will be returned as a <strong>Dataset/DataFrame</strong>.You can also interact with the SQL interface using the <strong>command-line</strong> or over <strong>JDBC/ODBC</strong>.</p>\n",
    "\n",
    "<h2>Datasets and DataFrames</h2>\n",
    "\n",
    "<p>A Dataset is a distributed collection of data.Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL&#8217;s optimized execution engine. A Dataset can be <strong>constructed</strong> from JVM objects and then\n",
    "manipulated using functional transformations (<code>map</code>, <code>flatMap</code>, <code>filter</code>, etc.).\n",
    "The Dataset API is available in <a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\">Scala</a> and\n",
    "<a href=\"http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\">Java</a>. Python does not have the support for the Dataset API. But due to Python&#8217;s dynamic nature,many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally<code>row.columnName</code>). The case for R is similar.</p>\n",
    "\n",
    "<p>A DataFrame is a <em>Dataset</em> organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood(在幕后). DataFrames can be constructed from a wide array of <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources\">sources</a> such as: <u>structured data files, tables in Hive, external databases, or existing RDDs</u>.The DataFrame API is available in Scala,Java, <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\">Python</a>, and <a href=\"http://spark.apache.org/docs/latest/api/R/index.html\">R</a>.In Scala and Java, a DataFrame is represented by a Dataset of <code>Row</code>s.In <a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\">the Scala API</a>, <code>DataFrame</code> is simply a type alias of <code>Dataset[Row]</code>.While, in <a href=\"http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\">Java API</a>, users need to use <code>Dataset&lt;Row&gt;</code> to represent a <code>DataFrame</code>.</p>\n",
    "\n",
    "<p>Throughout this document, we will often refer to Scala/Java Datasets of <code>Row</code>s as DataFrames.</p>\n",
    "\n",
    "<h1>Getting Started</h1>\n",
    "<h2>Starting Point: SparkSession</h2>\n",
    "<p>The entry point into(入口点) all functionality in Spark is the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession\"><code>SparkSession</code></a> class. To create a basic <code>SparkSession</code>, just use <code>SparkSession.builder</code>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"PythonSQL\")\\\n",
    ".config(\"spark.some.config.option\", 'some-value')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><code>SparkSession</code> in Spark 2.0 provides builtin support for Hive features including the ability to write queries using HiveQL, access to Hive UDFs(User defined Function,用户自定义函数), and the ability to read data from Hive tables.To use these features, you do not need to have an existing Hive setup.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"creating-dataframes\">Creating DataFrames</h2>\n",
    "<p>With a <code>SparkSession</code>, applications can create DataFrames from an <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds\">existing <code>RDD</code></a>,from a Hive table, or from <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources\">Spark data sources</a>.</p>\n",
    "\n",
    "<p>As an example, the following creates a DataFrame based on the content of a JSON file:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
