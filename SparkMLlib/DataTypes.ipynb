{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark版本为2.0.0,Python版本为3.5.2,Jupyter notebook server版本为4.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 class=\"title\">Data Types - RDD-based API</h1>\n",
    "<ul id=\"markdown-toc\">\n",
    "  <li>Local vector</li>\n",
    "  <li>Labeled point</li>\n",
    "  <li>Local matrix</li>\n",
    "  <li>Distributed matrix  \n",
    "      <ul>\n",
    "          <li>RowMatrix</li>\n",
    "          <li>IndexedRowMatrix</li>\n",
    "          <li>CoordinateMatrix</li>\n",
    "          <li>BlockMatrix</li>\n",
    "      </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "<p>MLlib supports local vectors and matrices stored on a single machine, as well as distributed matrices backed by one or more RDDs.Local vectors and local matrices are simple data models that serve as public interfaces. The underlying linear algebra operations are provided by <a href=\"http://www.scalanlp.org/\">Breeze</a>.A training example used in supervised learning is called a &#8220;labeled point&#8221; in MLlib.</p>\n",
    "<h2>Local vector</h2>\n",
    "<p>A local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine.  MLlib supports two types of local vectors: dense and sparse.  A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values.  For example, a vector <code>(1.0, 0.0, 3.0)</code> can be represented in dense format as <code>[1.0, 0.0, 3.0]</code> or in sparse format as <code>(3, [0, 2], [1.0, 3.0])</code>, where <code>3</code> is the size of the vector.</p>\n",
    "<p>稀疏向量和密集向量都是向量的表示方法.<br/>密集向量和稀疏向量的区别:密集向量的值就是一个普通的Double数组,而稀疏向量由两个并列的数组indices和values组成.例如:向量(1.0,0.0,1.0,3.0)用密集向量表示为[1.0,0.0,1.0,3.0],用稀疏向量表示为(4,[0,2,3],[1.0,1.0,3.0]).第一个4表示向量的长度(元素个数),[0,2,3]就是indices数组,[1.0,1.0,3.0]是values数组,表示向量0的位置的值是1.0，1的位置的值是1.0,而3的位置的值是3.0,其他的位置都是0.</p>\n",
    "<p>MLlib recognizes(识别) the following types as dense vectors:</p>\n",
    "<ul>\n",
    "   <li>NumPy&#8217;s <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html\"><code>array</code></a></li>\n",
    "   <li>Python&#8217;s list, e.g., <code>[1, 2, 3]</code></li>\n",
    "</ul>\n",
    "<p>and the following as sparse vectors:</p>\n",
    "<ul>\n",
    "    <li>MLlib&#8217;s <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector\"><code>SparseVector</code></a>.</li>\n",
    "    <li>SciPy&#8217;s<a href=\"http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix\"><code>csc_matrix</code></a>with a single column</li>\n",
    "</ul>\n",
    "<p>We recommend using NumPy arrays over lists for efficiency, and using the factory methods implemented\n",
    "in <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors\"><code>Vectors</code></a> to create sparse vectors.</p>为了提高效率推荐使用NumPy而不是list创建dense vectors，使用MLlib内置的Vectors方法创建sparse vectors.\n",
    "<p>Refer to the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors\"><code>Vectors</code> Python docs</a> for more details on the API.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# Use a NumPy array as a dense vector.\n",
    "dv1 = np.array([1.0, 0.0, 3.0])\n",
    "# Use a Python list as a dense vector.\n",
    "dv2 = [1.0, 0.0, 3.0]\n",
    "# Create a SparseVector.\n",
    "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n",
    "# Use a single-column SciPy csc_matrix as a sparse vector.\n",
    "sv2 = sps.csc_matrix((np.array([1.0, 3.0]), np.array([0, 2]), np.array([0, 2])), shape = (3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  3.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0, 3.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  3.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv1.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [ 3.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv2 .toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csc_matrix:Compressed Sparse Column matrix（压缩稀疏列矩阵）\n",
    ">csc_matrix((data, indices, indptr), [shape=(M, N)])<br/>&emsp;&emsp;is the standard CSC representation where the row indices for column i are stored in <strong>`indices[indptr[i]:indptr[i+1]]`</strong> and their corresponding values are stored in <strong>`data[indptr[i]:indptr[i+1]]`</strong>. If the shape parameter is not supplied, the matrix dimensions are inferred from the index arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 4],\n",
       "       [0, 0, 5],\n",
       "       [2, 3, 6]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indptr = np.array([0, 2, 3, 6])\n",
    "indices = np.array([0, 2, 2, 0, 1, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "sps.csc_matrix((data, indices, indptr),shape=(3, 3)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于0列来说,indptr[0]:indptr[1]=[0,1],再看行indices[0:2]=[0,2],数据data[0:2]=[1,2],说明第0列在第0行和第2行上有数据1和2.<br/>\n",
    "对于1列来说,indptr[1]:indptr[2]=[2],再看行indices[2]=[2],数据data[2]=[3],说明第1列在第2行上有数据3.<br/>\n",
    "对于2列来说,indptr[2]:indptr[3]=[3:6],再看行indices[3:6]=[0,1,2],数据data[3:6]=[4,5,6],说明第2列在第0,1,2行上有数据4,5,6.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Labeled point</h2>\n",
    "<p>A labeled point is represented by <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint\"><code>LabeledPoint</code></a>.</p>\n",
    "<p>Refer to the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint\"><code>LabeledPoint</code> Python docs</a> for more details on the API.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Create a labeled point with a positive label and a dense feature vector.\n",
    "pos = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n",
    "\n",
    "# Create a labeled point with a negative label and a sparse feature vector.\n",
    "neg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dt>\n",
    "<em>class </em>\n",
    "<tt>pyspark.mllib.regression.</tt>\n",
    "<tt>LabeledPoint</tt>\n",
    "<big>(</big><em>label</em>, <em>features</em><big>)</big>\n",
    "</dt>\n",
    "\n",
    "<table>\n",
    "<col/>\n",
    "<col/>\n",
    "<tbody>\n",
    "<tr>\n",
    "    <th>Parameters:</th>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li><strong>label</strong> &#8211; Label for this data point.</li>\n",
    "            <li><strong>features</strong> &#8211; Vector of features for this point (NumPy array,list,pyspark.mllib.linalg.SparseVector, or scipy.sparse column matrix).</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<p>Note: &#8216;label&#8217; and &#8216;features&#8217; are accessible as class attributes.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 0.0, 3.0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong><em>Sparse data</em></strong></p>\n",
    "\n",
    "<p>It is very common in practice to have sparse training data.  MLlib supports reading training\n",
    "examples stored in <code>LIBSVM</code> format, which is the default format used by <a href=\"http://www.csie.ntu.edu.tw/~cjlin/libsvm/\"><code>LIBSVM</code></a> and <a href=\"http://www.csie.ntu.edu.tw/~cjlin/liblinear/\"><code>LIBLINEAR</code></a>.  It is a text format in which each line represents a labeled sparse feature vector using the following format:</p>\n",
    "<pre><code>label index1:value1 index2:value2 ...</code></pre>\n",
    "<p>where the indices are `one-based` and in `ascending order`. After loading, the feature indices are converted to `zero-based`.</p>\n",
    "<p><a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.util.MLUtils.loadLibSVMFile\"><code>MLUtils.loadLibSVMFile</code></a> reads training examples stored in LIBSVM format.</p>\n",
    "<p>Refer to the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.util.MLUtils\"><code>MLUtils</code> Python docs</a> for more details on the API.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">对于如下的LIBSVM格式数据：<br/>\n",
    "<pre><code>        0 128:51 129:159 130:253 131:159 132:50</code></pre>\n",
    "0.0代表标签,128代表索引,51代表特征值。<br/><br/>\n",
    "经MLUtils.loadLibSVMFile读取后格式转换为如下：<br/>\n",
    "<pre><code>        (0.0,(5,[127,128,129,130,131],[51.0,159.0,253.0,159.0,50.0]))</code></pre>\n",
    "0.0代表标签，5代表数据个数，[127,128,129,130,131]代表特征位置索引，[51.0,159.0,253.0,159.0,50.0]代表特征值。<br/><br/>\n",
    "注意：索引(index)从1开始顺序递增，读取完成后，特征索引被转换成从0开始。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dt>\n",
    "<em>static </em>\n",
    "    <tt>loadLibSVMFile</tt>\n",
    "    <big>(</big><em>sc</em>, <em>path</em>, <em>numFeatures=-1</em>, <em>minPartitions=None</em>, <em>multiclass=None</em><big>)</big>\n",
    "</dt>\n",
    "<tbody>\n",
    "    <tr>\n",
    "        <th>Parameters:</th>\n",
    "        <td>\n",
    "            <ul>\n",
    "                <li><strong>sc</strong> – Spark context</li>\n",
    "                <li><strong>path</strong> – file or directory path in any Hadoop-supported file\n",
    "system URI</li>\n",
    "                <li><strong>numFeatures</strong> – number of features, which will be determined from the input data if a nonpositive value is given. This is useful when the dataset is already split into multiple files and you want to load them separately, because some features may not present in certain files, which leads to inconsistent feature\n",
    "dimensions.</li>\n",
    "                <li><strong>minPartitions</strong> – min number of partitions</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Returns:</th>\n",
    "        <td><p>labeled data stored as an RDD of LabeledPoint</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "</tbody>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "examples = MLUtils.loadLibSVMFile(sc, \"mllib/sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(0.0, (692,[127,128,129,130,131,154,155,156,157,158,159,181,182,183,184,185,186,187,188,189,207,208,209,210,211,212,213,214,215,216,217,235,236,237,238,239,240,241,242,243,244,245,262,263,264,265,266,267,268,269,270,271,272,273,289,290,291,292,293,294,295,296,297,300,301,302,316,317,318,319,320,321,328,329,330,343,344,345,346,347,348,349,356,357,358,371,372,373,374,384,385,386,399,400,401,412,413,414,426,427,428,429,440,441,442,454,455,456,457,466,467,468,469,470,482,483,484,493,494,495,496,497,510,511,512,520,521,522,523,538,539,540,547,548,549,550,566,567,568,569,570,571,572,573,574,575,576,577,578,594,595,596,597,598,599,600,601,602,603,604,622,623,624,625,626,627,628,629,630,651,652,653,654,655,656,657],[51.0,159.0,253.0,159.0,50.0,48.0,238.0,252.0,252.0,252.0,237.0,54.0,227.0,253.0,252.0,239.0,233.0,252.0,57.0,6.0,10.0,60.0,224.0,252.0,253.0,252.0,202.0,84.0,252.0,253.0,122.0,163.0,252.0,252.0,252.0,253.0,252.0,252.0,96.0,189.0,253.0,167.0,51.0,238.0,253.0,253.0,190.0,114.0,253.0,228.0,47.0,79.0,255.0,168.0,48.0,238.0,252.0,252.0,179.0,12.0,75.0,121.0,21.0,253.0,243.0,50.0,38.0,165.0,253.0,233.0,208.0,84.0,253.0,252.0,165.0,7.0,178.0,252.0,240.0,71.0,19.0,28.0,253.0,252.0,195.0,57.0,252.0,252.0,63.0,253.0,252.0,195.0,198.0,253.0,190.0,255.0,253.0,196.0,76.0,246.0,252.0,112.0,253.0,252.0,148.0,85.0,252.0,230.0,25.0,7.0,135.0,253.0,186.0,12.0,85.0,252.0,223.0,7.0,131.0,252.0,225.0,71.0,85.0,252.0,145.0,48.0,165.0,252.0,173.0,86.0,253.0,225.0,114.0,238.0,253.0,162.0,85.0,252.0,249.0,146.0,48.0,29.0,85.0,178.0,225.0,253.0,223.0,167.0,56.0,85.0,252.0,252.0,252.0,229.0,215.0,252.0,252.0,252.0,196.0,130.0,28.0,199.0,252.0,252.0,253.0,252.0,252.0,233.0,145.0,25.0,128.0,252.0,253.0,252.0,141.0,37.0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Local matrix</h2>\n",
    "\n",
    "<p>A local matrix has integer-typed row and column indices and double-typed values, stored on a single\n",
    "machine.  MLlib supports dense matrices, whose entry values are stored in a single double array in\n",
    "column-major order, and sparse matrices, whose non-zero entry values are stored in the Compressed Sparse\n",
    "Column (CSC) format in `column-major order`(以列为主序).  For example, the following dense matrix\n",
    "$$\\begin{pmatrix}\n",
    "1.0 & 2.0 \\\\\n",
    "3.0 & 4.0 \\\\\n",
    "5.0 & 6.0 \\\\\n",
    "\\end{pmatrix}$$\n",
    "is stored in a one-dimensional array <code>[1.0, 3.0, 5.0, 2.0, 4.0, 6.0]</code> with the matrix size <code>(3, 2)</code>.</p>\n",
    "<p>The base class of local matrices is <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Matrix\"><code>Matrix</code></a>, and we provide two implementations: <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.DenseMatrix\"><code>DenseMatrix</code></a>,and <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseMatrix\"><code>SparseMatrix</code></a>.\n",
    "We recommend using the factory methods implemented in <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Matrices\"><code>Matrices</code></a> to create local\n",
    "matrices. <strong>Remember, local matrices in MLlib are stored in `column-major order`(列主序)</strong>.</p>\n",
    "<p>Refer to the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Matrix\"><code>Matrix</code> Python docs</a> and <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Matrices\"><code>Matrices</code> Python docs</a> for more details on the API.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "稀疏矩阵是指矩阵中的元素大部分是0的矩阵，事实上，实际问题中大规模矩阵基本上都是稀疏矩阵，很多稀疏度在90%甚至99%以上。因此我们需要有高效的稀疏矩阵存储格式。几种典型的格式：COO,CSR,CSS。<br/>\n",
    "(1)Coordinate(COO)<br/>\n",
    "<img src=\"imgs/DataTypes_01.png\">\n",
    "这是最简单的一种格式，每一个元素需要用一个三元组来表示，分别是(行号，列号，数值)，对应上图右边的一列。这种方式简单，但是记录单信息多(行列)，每个三元组自己可以定位，因此空间不是最优。<br/>\n",
    "(2)Compressed Sparse Row(CSR)<br/>\n",
    "<img src=\"imgs/DataTypes_02.png\">\n",
    "CSR是比较标准的一种，也需要三类数据来表达：数值，列号，以及行偏移。CSR不是三元组，而是整体的编码方式。数值和列号与COO一致，表示一个元素以及其列号，行偏移表示某一行的第一个元素在values里面的起始偏移位置。<br/>如上图中，第一行元素1是0偏移，第二行元素2是2偏移，第三行元素5是4偏移，第4行元素6是7偏移。在行偏移的最后补上矩阵总的元素个数，本例中是9。<br/>\n",
    "(3)Compressed Sparse Column(CSC)<br/>\n",
    "CSC是和CSR相对应的一种方式，即按列压缩的意思。<br/>\n",
    "以上图中矩阵为例：<br/>\n",
    "Values：        [1 5 7 2 6 8 3 9 4]<br/>\n",
    "Row Indices：[0 2 0 1 3 1 2 2 3]<br/>\n",
    "Column Offsets：[0 2 5 7 9]<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "\n",
    "# Create a dense matrix ((1.0, 4.0), (4.0, 5.0), (3.0, 6.0))\n",
    "dm2 = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.],\n",
       "       [ 2.,  5.],\n",
       "       [ 3.,  6.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm2.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.,  0.],\n",
       "       [ 0.,  8.],\n",
       "       [ 0.,  6.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Distributed matrix(分布式矩阵)</h2>\n",
    "<p>A distributed matrix has long-typed row and column indices and double-typed values, stored distributively in one or more RDDs. It is very important to choose the right format to store large and distributed matrices.Converting a distributed matrix to a different format may require a global shuffle, which is quite expensive.Four types of distributed matrices have been implemented so far.</p>\n",
    "<p>The basic type is called <code>RowMatrix</code>. A <code>RowMatrix</code> is a row-oriented(面向行型) distributed\n",
    "matrix without meaningful row indices, e.g., a collection of feature vectors.It is backed by an RDD of its rows, where each row is a local vector.We assume that the number of columns is not huge for a <code>RowMatrix</code> so that a single local vector can be reasonably(合理地;相当地;适度地) communicated to the driver and can also be stored/operated on using a single node. An <code>IndexedRowMatrix</code> is similar to a <code>RowMatrix</code> but with row indices,which can be used for identifying rows and executing joins.A <code>CoordinateMatrix</code> is a distributed matrix stored in <a href=\"https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_.28COO.29\">coordinate list (COO)</a> format,backed by an RDD of its entries.A <code>BlockMatrix</code> is a distributed matrix backed by an RDD of <code>MatrixBlock</code>which is a tuple of <code>(Int, Int, Matrix)</code>.</p><p><strong><em>Note</em></strong></p>\n",
    "<p>The underlying RDDs of a distributed matrix must be deterministic, because we cache the matrix size.In general the use of non-deterministic RDDs can lead to errors.</p>\n",
    "<p>分布式矩阵如下所示:</p>\n",
    "<img src=\"imgs/DataTypes_03.png\">\n",
    "<h3 id=\"rowmatrix\">RowMatrix(行矩阵)</h3>\n",
    "<p>A <code>RowMatrix</code> is a row-oriented distributed matrix without meaningful row indices, backed by an RDD\n",
    "of its rows, where each row is a local vector.Since each row is represented by a local vector,the number of columns is limited by the integer range but it should be much smaller in practice.</p>\n",
    "<p>A <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.RowMatrix\"><code>RowMatrix</code></a> can be created from an <code>RDD</code> of vectors.</p>\n",
    "<p>Refer to the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.RowMatrix\"><code>RowMatrix</code> Python docs</a> for more details on the API.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RowMatrix直接通过RDD[Vector]来定义并可以用来统计平均数、方差、协同方差等。分布式行矩阵就是把每行对应一个RDD，将矩阵的每行分布式存储，矩阵的每行是一个本地向量。这和多变量统计的数据矩阵比较相似。因为每行以一个本地向量表示，所以矩阵列的数量被限制在整数范围内使用，但是在实际应用中列数很小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "# Create an RDD of vectors.\n",
    "rows = sc.parallelize([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Create a RowMatrix from an RDD of vectors.\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Get its size.\n",
    "m = mat.numRows()  # 4\n",
    "n = mat.numCols()  # 3\n",
    "\n",
    "# Get the rows as an RDD of vectors again.\n",
    "rowsRDD = mat.rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 2.0, 3.0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowsRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>IndexedRowMatrix(行索引矩阵)</h3>\n",
    "<p>An <code>IndexedRowMatrix</code> is similar to a <code>RowMatrix</code> but with meaningful row indices.  It is backed by an RDD of indexed rows, so that each row is represented by its index (long-typed) and a local \n",
    "vector.</p>\n",
    "<p>An <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix\"><code>IndexedRowMatrix</code></a> can be created from an <code>RDD</code> of <code>IndexedRow</code>s, where \n",
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRow\"><code>IndexedRow</code></a> is a wrapper over <code>(long, vector)</code>.An <code>IndexedRowMatrix</code> can be converted to a <code>RowMatrix</code> by dropping its row indices.</p>\n",
    "<p>Refer to the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix\"><code>IndexedRowMatrix</code> Python docs</a> for more details on the API.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>\n",
    "<em>class </em><tt>pyspark.mllib.linalg.distributed.</tt><tt><span>IndexedRow</span></tt><big>(</big><em>index</em>, <em>vector</em><big>)</big></dt>\n",
    "<dd><p>Bases: <tt><span class=\"pre\">object</span></tt></p>\n",
    "<p>Represents a row of an <span>IndexedRow</span>Matrix.</p>\n",
    "<p>Just a wrapper over a (long, vector) tuple.</p>\n",
    "<table>\n",
    "<colgroup><col>\n",
    "<col>\n",
    "</colgroup><tbody valign=\"top\">\n",
    "<tr><th>Parameters:</th><td><ul>\n",
    "<li><strong>index</strong> – The index for the given row.</li>\n",
    "<li><strong>vector</strong> – The row in the matrix at the given index.</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IndexedRowMatrix和RowMatrix相似，区别是它带有有一定意义的行索引。在RowMatrix中，rows的格式是RDD[Vector]；而在IndexedRowMatrix中，rows的格式是RDD[IndexedRow]，其中IndexedRow(index:Long,vector:Vector),相比RowMatrix index索引信息。一个IndexedRowMatrix可以从RDD[IndexedRow]实例创建，IndexedRow是(Int,Vector)的wrapper，而且这种矩阵可以转换成RowMatrix，从而利用其统计功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "\n",
    "# Create an RDD of indexed rows.\n",
    "# - This can be done explicitly with the IndexedRow class:\n",
    "indexedRows = sc.parallelize([IndexedRow(0, [1, 2, 3]), \n",
    "                              IndexedRow(1, [4, 5, 6]), \n",
    "                              IndexedRow(2, [7, 8, 9]), \n",
    "                              IndexedRow(3, [10, 11, 12])])\n",
    "# - or by using (long, vector) tuples:\n",
    "indexedRows = sc.parallelize([(0, [1, 2, 3]), (1, [4, 5, 6]), \n",
    "                              (2, [7, 8, 9]), (3, [10, 11, 12])])\n",
    "\n",
    "# Create an IndexedRowMatrix from an RDD of IndexedRows.\n",
    "mat = IndexedRowMatrix(indexedRows)\n",
    "\n",
    "# Get its size.\n",
    "m = mat.numRows()  # 4\n",
    "n = mat.numCols()  # 3\n",
    "\n",
    "# Get the rows as an RDD of IndexedRows.\n",
    "rowsRDD = mat.rows\n",
    "\n",
    "# Convert to a RowMatrix by dropping the row indices.\n",
    "rowMat = mat.toRowMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IndexedRow(0, [1.0,2.0,3.0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowsRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 2.0, 3.0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowMat.rows.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>CoordinateMatrix(坐标矩阵)</h3>\n",
    "\n",
    "<p>A <code>CoordinateMatrix</code> is a distributed matrix backed by an RDD of its entries.  Each entry is a tuple\n",
    "of <code>(i: Long, j: Long, value: Double)</code>, where <code>i</code> is the row index, <code>j</code> is the column index, and <code>value</code> is the entry value.  A <code>CoordinateMatrix</code> should be used only when both dimensions of the matrix are huge and the matrix is very sparse.</p>\n",
    "<p>A <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.CoordinateMatrix\"><code>CoordinateMatrix</code></a>\n",
    "can be created from an <code>RDD</code> of <code>MatrixEntry</code> entries, where \n",
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.MatrixEntry\"><code>MatrixEntry</code></a> is a wrapper over <code>(long, long, float)</code>.  A <code>CoordinateMatrix</code> can be converted to a <code>RowMatrix</code> by calling <code>toRowMatrix</code>, or to an <code>IndexedRowMatrix</code> with sparse rows by calling <code>toIndexedRowMatrix</code>.</p>\n",
    "<p>Refer to the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.CoordinateMatrix\"><code>CoordinateMatrix</code> Python docs</a> for more details on the API.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>\n",
    "<em>class </em><tt>pyspark.mllib.linalg.distributed.</tt><tt><span>MatrixEntry</span></tt><big>(</big><em>i</em>, <em>j</em>, <em>value</em><big>)</big></dt>\n",
    "<dd><p>Bases: <tt><span>object</span></tt></p>\n",
    "<p>Represents an entry of a CoordinateMatrix.</p>\n",
    "<p>Just a wrapper over a (long, long, float) tuple.</p>\n",
    "<table>\n",
    "<colgroup><col>\n",
    "<col>\n",
    "</colgroup><tbody valign=\"top\">\n",
    "<tr><th>Parameters:</th><td><ul>\n",
    "<li><strong>i</strong> – The row index of the matrix.</li>\n",
    "<li><strong>j</strong> – The column index of the matrix.</li>\n",
    "<li><strong>value</strong> – The (i, j)th entry of the matrix, as a float.</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</dd></dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "坐标矩阵常用于稀疏性比较高的计算中，每一项都是一个(i: Long, j: Long, value: Float)指示行列值的元组tuple。其中i是行坐标，j是列坐标，value是值。如果矩阵是非常大的而且稀疏，则坐标矩阵一定是最好的选择。坐标矩阵是通过RDD[MatrixEntry]实例创建的，MatrixEntry为(Long,Long,Float)形式，坐标矩阵可以转化为IndexedRowMatrix。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "\n",
    "# Create an RDD of coordinate entries.\n",
    "# - This can be done explicitly with the MatrixEntry class:\n",
    "entries = sc.parallelize([MatrixEntry(0, 0, 1.2), MatrixEntry(1, 0, 2.1), MatrixEntry(6, 1, 3.7)])\n",
    "# - or using (long, long, float) tuples:\n",
    "entries = sc.parallelize([(0, 0, 1.2), (1, 0, 2.1), (2, 1, 3.7)])\n",
    "\n",
    "# Create an CoordinateMatrix from an RDD of MatrixEntries.\n",
    "mat = CoordinateMatrix(entries)\n",
    "\n",
    "# Get its size.\n",
    "m = mat.numRows()  # 3\n",
    "n = mat.numCols()  # 2\n",
    "\n",
    "# Get the entries as an RDD of MatrixEntries.\n",
    "entriesRDD = mat.entries\n",
    "\n",
    "# Convert to a RowMatrix.\n",
    "rowMat = mat.toRowMatrix()\n",
    "\n",
    "# Convert to an IndexedRowMatrix.\n",
    "indexedRowMat = mat.toIndexedRowMatrix()\n",
    "\n",
    "# Convert to a BlockMatrix.\n",
    "blockMat = mat.toBlockMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MatrixEntry(0, 0, 1.2), MatrixEntry(1, 0, 2.1), MatrixEntry(2, 1, 3.7)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entriesRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(2, {0: 1.2})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowMat.rows.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IndexedRow(0, (2,[0],[1.2]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexedRowMat.rows.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>BlockMatrix(分块矩阵)</h3>\n",
    "<p>A <code>BlockMatrix</code> is a distributed matrix backed by an RDD of <code>MatrixBlock</code>s, where a <code>MatrixBlock</code> is a tuple of <code>((Int, Int), Matrix)</code>, where the <code>(Int, Int)</code> is the index of the block, and <code>Matrix</code> is the sub-matrix at the given index with size<code>rowsPerBlock</code> x <code>colsPerBlock</code>.<code>BlockMatrix</code> supports methods such as <code>add</code> and <code>multiply</code> with another <code>BlockMatrix</code>.<code>BlockMatrix</code> also has a helper function <code>validate</code> which can be used to check whether the <code>BlockMatrix</code> is set up properly.</p>\n",
    "<p>A <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.BlockMatrix\"><code>BlockMatrix</code></a>can be created from an <code>RDD</code> of sub-matrix blocks, where a sub-matrix block is a \n",
    "<code>((blockRowIndex, blockColIndex), sub-matrix)</code> tuple.</p>\n",
    "<p>Refer to the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.BlockMatrix\"><code>BlockMatrix</code> Python docs</a> for more details on the API.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数学理论中，一个分块矩阵或是分段矩阵就是将矩阵分割出较小的矩形矩阵，这些较小的矩阵就被称为区块。换个方式来说，就是以较小的矩阵组合成一个矩阵。分块矩阵的分割原则是以水平线和垂直线进行划分。在分块矩阵中，位于同一行(列)的每一个子矩阵都拥有相同的列数(行数)。通过将大的矩阵以分块的方式划分，并将每个分块看作另一个矩阵的元素，这样之后再参与运算，通常可以让计算变得清晰甚至得以大幅简化。例如，有的大矩阵可以通过分块变为对角矩阵或者是三角矩阵等特殊形式的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.linalg.distributed import BlockMatrix\n",
    "\n",
    "# Create an RDD of sub-matrix blocks.\n",
    "blocks = sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])), \n",
    "                         ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))])\n",
    "\n",
    "# Create a BlockMatrix from an RDD of sub-matrix blocks.\n",
    "mat = BlockMatrix(blocks, 3, 2)\n",
    "\n",
    "# Get its size.\n",
    "m = mat.numRows() # 6\n",
    "n = mat.numCols() # 2\n",
    "\n",
    "# Get the blocks as an RDD of sub-matrix blocks.\n",
    "blocksRDD = mat.blocks\n",
    "\n",
    "# Convert to a LocalMatrix.\n",
    "localMat = mat.toLocalMatrix()\n",
    "\n",
    "# Convert to an IndexedRowMatrix.\n",
    "indexedRowMat = mat.toIndexedRowMatrix()\n",
    "\n",
    "# Convert to a CoordinateMatrix.\n",
    "coordinateMat = mat.toCoordinateMatrix()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
