{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark版本为2.0.0,Python版本为3.5.2,Jupyter notebook server版本为4.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png\">\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png\">\n",
    "<h1>Spark Tutorial: Learning Apache Spark</h1>\n",
    "<p>This tutorial will teach you how to use [Apache Spark](http://spark.apache.org/), a framework for large-scale data processing, within a notebook. Many traditional frameworks were designed to be run on a single computer.  However, many datasets today are too large to be stored on a single computer, and even when a dataset can be stored on one computer (such as the datasets in this tutorial), the dataset can often be processed much more quickly using multiple computers.</p>\n",
    "<p>Spark has efficient implementations of a number of <em>transformations</em> and <em>actions</em> that can be composed together to perform data processing and analysis.  Spark excels at(在某一活动方面表现杰出，擅长于某项活动) distributing these operations across a cluster while(同时) abstracting away(抽象) many of the underlying implementation details(底层的实现细节).  Spark has been designed with a focus on scalability and efficiency.  With Spark you can begin developing your solution on your laptop, using a small dataset, and then use that same code to process terabytes or even petabytes across a distributed cluster.</p>\n",
    "<p>During this tutorial we will cover:\n",
    "<ul>\n",
    "<li>Part 1:Basic notebook usage and [Python](https://docs.python.org/2/) integration</li>\n",
    "<li>Part 2:An introduction to using [Apache Spark](https://spark.apache.org/) with the [PySpark SQL API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module) running in a notebook</li>\n",
    "<li>Part 3:Using DataFrames and chaining together transformations and actions</li>\n",
    "<li>Part 4:Python Lambda functions and User Defined Functions</li>\n",
    "<li>Part 5:Additional DataFrame actions</li>\n",
    "<li>Part 6:Additional DataFrame transformations</li>\n",
    "<li>Part 7:Caching DataFrames and storage options</li>\n",
    "<li>Part 8:Debugging Spark applications and lazy evaluation</li>\n",
    "</ul></p>\n",
    "<p>The following transformations will be covered:\n",
    "<ul><li><strong>select(), filter(), distinct(), dropDuplicates(), orderBy(), groupBy()</strong></li></ul></p>\n",
    "<p>The following actions will be covered:\n",
    "<ul><li><strong>first(), take(), count(), collect(), show()</strong></li></ul></p>\n",
    "<p>Also covered:\n",
    "<ul><li><strong>cache(), unpersist()</strong></li></ul></p>\n",
    "<p>Note that, for reference, you can look up the details of these methods in the [Spark's PySpark SQL API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module).</p>\n",
    "<h2>Part 1: Basic notebook usage and [Python](https://docs.python.org/2/) integration</h2>\n",
    "<h3>(1a) Notebook usage</h3>\n",
    "<p>A notebook is comprised of a linear sequence of cells.  These cells can contain either <em>markdown</em> or <em>code</em>, but we won't mix both in one cell.  When a markdown cell is executed it renders formatted text, images, and links just like HTML in a normal webpage.  The text you are reading right now is part of a markdown cell.  <em>Python code</em> cells allow you to execute arbitrary Python commands just like in any Python shell. Place your cursor inside the cell below, and press <strong>\"Shift\" + \"Enter\"</strong> to execute the code and advance to the next cell.  You can also press <strong>\"Ctrl\" + \"Enter\"</strong> to execute the code and remain in the cell.  These commands work the same in both markdown and code cells.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of 1 and 1 is 2\n"
     ]
    }
   ],
   "source": [
    "# This is a Python cell. You can run normal Python code here...\n",
    "print('The sum of 1 and 1 is {0}'.format(1+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of 1 and 2 is 3\n"
     ]
    }
   ],
   "source": [
    "# Here is another Python cell, this time with a variable (x) declaration and an if statement:\n",
    "x = 42\n",
    "if x > 40:\n",
    "    print ('The sum of 1 and 2 is {0}'.format(1+2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>(1b) Notebook state</h3>\n",
    "<p>As you work through a notebook it is important that you run all of the code cells.  The notebook is stateful(有状态的), which means that variables and their values are retained until the kernel is restarted  in Jupyter notebooks.  If you do not run all of the code cells as you proceed through the notebook, your variables will not be properly initialized and later code might fail.  You will also need to rerun any cells that you have modified in order for the changes to be available to other cells.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "# This cell relies on x being defined already.\n",
    "# If we didn't run the cells from part (1a) this code would fail.\n",
    "print (x * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>(1c) Library imports</h2>\n",
    "<p>We can import standard Python libraries ([modules](https://docs.python.org/2/tutorial/modules.html)) the usual way.  An `import` statement will import the specified module.  In this tutorial and future labs, we will provide any imports that are necessary.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the regular expression library\n",
    "import re\n",
    "m = re.search('(?<=abc)def', 'abcdef')\n",
    "m.group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?:pattern)**<br/>\n",
    "&emsp;&emsp;匹配pattern但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用或字符“(|)”来组合一个模式的各个部分是很有用的。例如“industr(?:y|ies)”就是一个比“industry|industries”更简略的表达式。<br/>\n",
    "**(?=pattern)**<br/>\n",
    "&emsp;&emsp;正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，“Windows（?=95|98|NT|2000）”能匹配“Windows2000”中的“Windows”，但不能匹配“Windows3.1”中的“Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。<br/>\n",
    "**(?!pattern)**<br/>\n",
    "&emsp;&emsp;正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如“Windows（?！95|98|NT|2000）”能匹配“Windows3.1”中的“Windows”，但不能匹配“Windows2000”中的“Windows”。<br/>\n",
    "**(?<=pattern)**<br/>\n",
    "&emsp;&emsp;反向肯定预查，与正向肯定预查类似，只是方向相反。例如，“（?<=95|98|NT|2000）Windows”能匹配“2000Windows”中的“Windows”，但不能匹配“3.1Windows”中的“Windows”。<br/>\n",
    "**(?<!pattern)**<br/>\n",
    "&emsp;&emsp;反向否定预查，与正向否定预查类似，只是方向相反。例如“(?<!95|98|NT|2000)Windows”能匹配“3.1Windows”中的“Windows”，但不能匹配“2000Windows”中的“Windows”。<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was last run on: 2016-09-29 23:55:22.238618\n"
     ]
    }
   ],
   "source": [
    "# Import the datetime library\n",
    "import datetime\n",
    "print ('This was last run on: {0}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 2: An introduction to using [Apache Spark](https://spark.apache.org/) with the [PySpark SQL API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module) running in a notebook</h1>\n",
    "<h2>Spark Context</h2>\n",
    "<p>In Spark, communication occurs between a `driver` and `executors`.  The `driver` has Spark jobs that it needs to run and these jobs are split into tasks that are submitted to the `executors` for completion.  The results from these tasks are delivered back to the `driver`.</p>\n",
    "<p>In part 1, we saw that normal Python code can be executed via cells. When using `Databricks` this code gets executed in the Spark driver's Java Virtual Machine (JVM) and not in an executor's JVM, and when using an `Jupyter notebook` it is executed within the kernel associated with the notebook. Since no Spark functionality is actually being used, no tasks are launched on the executors.</p>\n",
    "<p>In order to use Spark and its DataFrame API we will need to use a `SQLContext`.  When running Spark, you start a new Spark application by creating a [SparkContext](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext). You can then create a [SQLContext](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext) from the `SparkContext`. When the `SparkContext` is created, it asks the master for some cores to use to do work.  The master sets these cores aside just for you; they won't be used for other applications. When using Databricks, both a `SparkContext` and a `SQLContext` are created for you automatically. `sc` is your `SparkContext`, and `sqlContext` is your `SQLContext`.</p>\n",
    "<h3>(2a) Example Cluster</h3>\n",
    "<p>The diagram shows an example cluster, where the slots allocated for an application are outlined in purple(紫色的). (Note: We're using the term _slots_ here to indicate threads available to perform parallel work for Spark.</p>\n",
    "<p>Spark documentation often refers to these threads as _cores_, which is a confusing term, as the number of slots available on a particular machine does not necessarily have any relationship to the number of physical CPU\n",
    "cores on that machine.)</p>\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-2a.png\" style=\"height: 800px;float: right\"/>\n",
    "<p>You can view the details of your Spark application in the Spark web UI.  The web UI is accessible in Databricks by going to \"Clusters\" and then clicking on the \"Spark UI\" link for your cluster.  In the web UI, under the \"Jobs\" tab, you can see a list of jobs that have been scheduled or run.  It's likely there isn't any thing interesting here yet because we haven't run any jobs, but we'll return to this page later.</p>\n",
    "<p>At a high level, every Spark application consists of a `driver program` that launches various parallel operations on executor Java Virtual Machines (JVMs) running either in a cluster or locally on the same machine. In Databricks, \"Databricks Shell\" is the driver program.  When running locally, `pyspark` is the driver program. In all cases, this driver program contains the main loop for the program and creates distributed datasets on the cluster, then applies operations (transformations & actions) to those datasets.\n",
    "<strong>Driver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.</strong>A Spark SQL context object (`sqlContext`) is the main entry point for Spark DataFrame and SQL functionality. A `SQLContext` can be used to create DataFrames, which allows you to direct the operations on your data.</p>\n",
    "<p>Try printing out `sqlContext` to see its type.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.context.SQLContext"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the type of the Spark sqlContext\n",
    "type(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the type is `HiveContext`. This means we're working with a version of Spark that has Hive support. Compiling Spark with Hive support is a good idea, even if you don't have a Hive metastore. As the\n",
    "[Spark Programming Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#starting-point-sqlcontext) states, a `HiveContext` \"provides a superset(超集) of the functionality provided by the basic `SQLContext`. Additional features include the ability to write queries using the more complete HiveQL parser, access to Hive UDFs [user-defined functions], and the ability to read data from Hive tables. To use a `HiveContext`, you do not need to have an existing Hive setup, and all of the data sources available to a `SQLContext` are still available.\"\n",
    "<h3>(2b) SparkContext attributes</h3>\n",
    "<p>You can use Python's [dir()](https://docs.python.org/2/library/functions.html?highlight=dir#dir) function to get a list of all the attributes (including methods) accessible through the `sqlContext` object.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List sqlContext's attributes\n",
    "#dir(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Getting help\n",
    "\n",
    "Alternatively, you can use Python's [help()](https://docs.python.org/2/library/functions.html?highlight=help#help) function to get an easier to read list of all the attributes, including examples, that the `sqlContext` object has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use help to obtain more detailed information\n",
    "#help(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Outside of `pyspark` or a notebook, `SQLContext` is created from the lower-level `SparkContext`, which is usually used to create Resilient Distributed Datasets (RDDs). An RDD is the way Spark actually represents data internally(内部地); DataFrames are actually implemented in terms of RDDs.</p>\n",
    "<p>**While you can interact directly with RDDs, DataFrames are preferred.** They're generally faster, and they perform the same no matter what language (Python, R, Scala or Java) you use with Spark.</p>\n",
    "<p>**In this course, we'll be using DataFrames, so we won't be interacting directly with the Spark Context object very much.** However, it's worth knowing that inside `pyspark` or a notebook, you already have an existing `SparkContext` in the `sc` variable. One simple thing we can do with `sc` is check the version of Spark we're using:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After reading the help we've decided we want to use sc.version to see what version of Spark we are running\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过python的platform模块获取版本号:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dir(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Help can be used on any Python object\n",
    "#help(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 3: Using DataFrames and chaining together transformations and actions</h2>\n",
    "<h3>Working with your first DataFrames</h3>\n",
    "\n",
    "In Spark, we first create a base [DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame). We can then apply one or more transformations to that base DataFrame. **A DataFrame is immutable(不可变的), so once it is created, it cannot be changed. As a result(因此), each transformation creates a new DataFrame.** Finally, we can apply one or more actions to the DataFrames.\n",
    "\n",
    "> Note that Spark uses <em>lazy evaluation</em>, so transformations are not actually executed until an action occurs.\n",
    "\n",
    "We will perform several exercises to obtain a better understanding of DataFrames:\n",
    "* Create a Python collection of 10,000 integers\n",
    "* Create a Spark DataFrame from that collection\n",
    "* Subtract(减去) one from each value using `map`\n",
    "* Perform action `collect` to view results\n",
    "* Perform action `count` to view counts\n",
    "* Apply transformation `filter` and view results with `collect`\n",
    "* Learn about `lambda` functions\n",
    "* Explore how lazy evaluation works and the debugging challenges that it introduces\n",
    "\n",
    "A DataFrame consists of a series of **`Row`** objects; each **`Row`** object has a set of named columns. You can think of a DataFrame as modeling a table, though the data source being processed does not have to be a table.\n",
    "\n",
    "More formally(更正式一点的说法是), a DataFrame must have a **_schema_**, which means it must consist of columns, each of which has a _name_ and a _type_. Some data sources have schemas built into them. Examples include RDBMS databases, Parquet files, and NoSQL databases like Cassandra. Other data sources don't have computer-readable schemas, but you can often apply a schema programmatically.\n",
    "<h3>(3a) Create a Python collection of 10,000 people</h3>\n",
    "<p>We will use a third-party Python testing library called [fake-factory](https://pypi.python.org/pypi/fake-factory/0.5.3) to create a collection of fake(伪造的) person records.\n",
    "When using Faker for unit testing, you will often want to generate the same data set. The generator offers a seed() method, which seeds(设置随机种子发生器) the _random number generator_(随机数发生器). Calling the same script twice with the same seed produces the same results.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faker是一个可以让你生成伪造数据的Python包。当你需要初始化数据库，创建美观的XML文档，不断产生数据来进行压力测试或者想从生产服务器上拉取匿名数据的时候，Faker将是你最棒的选择。\n",
    "\n",
    "可以使用pip进行安装:\n",
    ">pip install fake-factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from faker import Factory\n",
    "fake = Factory.create()\n",
    "fake.seed(4321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use this factory to create a collection of randomly generated people records. In the next section, we'll turn that collection into a DataFrame. We'll use the Spark `Row` class, because that will help us define the Spark DataFrame schema. There are other ways to define schemas, though; see the Spark Programming Guide's discussion of [schema inference](http://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection) for more information. (For instance,we could also use a Python `namedtuple`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each entry(记录) consists of last_name, first_name, ssn(身份证号), job, and age (at least 1)\n",
    "from pyspark.sql import Row\n",
    "def fake_entry():\n",
    "  #type(fake.name()) --> unicode\n",
    "  #S.split([sep [,maxsplit]]) -> list of strings(If sep is not specified or is None, any whitespace string is a separator)\n",
    "  name = fake.name().split()\n",
    "  return (name[1], name[0], fake.ssn(), fake.job(), abs(2016 - fake.date_time().year) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a helper function to call a function repeatedly\n",
    "def repeat(times, func, *args, **kwargs):\n",
    "    for _ in range(times):\n",
    "        yield func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = list(repeat(10000, fake_entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data` is just a normal Python list, containing Python tuples objects. Let's look at the first item in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Brown', 'Jason', '182-83-5988', 'Community education officer', 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the size of the list using the Python `len()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>(3b) Distributed data and using a collection to create a DataFrame</h3>\n",
    "\n",
    "In Spark, datasets are represented as a list of entries, where the list is broken up into many different partitions that are each stored on a different machine.  Each partition holds a unique(唯一的)subset of the entries in the list.  Spark calls datasets that it stores \"Resilient Distributed Datasets\" (RDDs). Even DataFrames are ultimately(最终)represented as RDDs, with additional meta-data(元数据是用来描述数据的数据.(Data that describes other data)元数据最大的好处是，它使信息的描述和分类可以实现格式化，从而为机器处理创造了可能.)\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3b.png\" style=\"width: 900px; float: right; margin: 5px\"/>\n",
    "\n",
    "One of the defining features(定义性特征,本质特征) of Spark, compared to other data analytics frameworks (e.g., Hadoop), is that _**it stores data in memory rather than on disk**_.  This allows Spark applications to run much more quickly, because they are not slowed down by needing to read data from disk.\n",
    "The figure to the right illustrates how Spark breaks a list of data entries into partitions that are each stored in memory on a worker.\n",
    "\n",
    "\n",
    "To create the DataFrame, we'll use `sqlContext.createDataFrame()`, and we'll pass our array of data in as an argument to that function. Spark will create a new set of input data based on data that is passed in.  A DataFrame requires a _**schema**_, which is a list of columns, where each column has a name and a type. Our list of data has elements with types (mostly strings, but one integer). We'll supply the rest of the schema and the column names as the second argument to `createDataFrame()`.\n",
    "\n",
    "Let's view the help for `createDataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(sqlContext.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF = sqlContext.createDataFrame(data, ('last_name', 'first_name', 'ssn', 'occupation', 'age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what type `sqlContext.createDataFrame()` returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of dataDF: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print ('type of dataDF: {0}'.format(type(dataDF)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the DataFrame's schema and some of its rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can register the newly created DataFrame as a named table, using the `registerDataFrameAsTable()` method.\n",
    "\n",
    "**NOTE:**Registers the given DataFrame as a temporary table in the catalog.Temporary tables exist only during the lifetime of this instance of SQLContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(dataDF, 'dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What methods can we call on this DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(dataDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many partitions will the DataFrame be split into?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method getNumPartitions in module pyspark.rdd:\n",
      "\n",
      "getNumPartitions() method of pyspark.rdd.RDD instance\n",
      "    Returns the number of partitions in RDD\n",
      "    \n",
      "    >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      "    >>> rdd.getNumPartitions()\n",
      "    2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dataDF.rdd.getNumPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A note about DataFrames and queries\n",
    "\n",
    "When you use DataFrames or Spark SQL, you are building up a _**query plan**_(查询计划). Each transformation you apply to a DataFrame adds some information to the query plan. When you finally call an action, which triggers execution of your **Spark job**, several things happen:\n",
    "\n",
    "1. Spark's Catalyst optimizer analyzes the query plan (called an _unoptimized logical query plan_未经优化的逻辑查询计划) and attempts to optimize it. Optimizations include (but aren't limited to) rearranging and combining(重新排列和组合) `filter()` operations for efficiency, converting `Decimal` operations to more efficient long integer operations, and pushing some operations down into the data source (e.g., a `filter()` operation might be translated to a SQL `WHERE` clause, if the data source is a traditional SQL RDBMS). The result of this optimization phase(优化阶段) is an _optimized logical plan_(优化的逻辑计划).\n",
    "2. Once Catalyst has an optimized logical plan, it then constructs multiple _physical_ plans from it. Specifically, it implements the query in terms of lower level Spark RDD operations.\n",
    "3. Catalyst chooses which physical plan to use via _cost optimization_(成本最优化). That is, it determines which physical plan is the most efficient (or least expensive), and uses that one.\n",
    "4. Finally, once the physical RDD execution plan is established, Spark actually executes the job.\n",
    "\n",
    "You can examine the query plan using the `explain()` function on a DataFrame. By default, `explain()` only shows you the final physical plan; however, if you pass it an argument of `True`, it will show you all phases(所有的解析过程).\n",
    "\n",
    "(If you want to take a deeper dive into how Catalyst optimizes DataFrame queries, this blog post, while a little old, is an excellent overview: [Deep Dive into Spark SQL's Catalyst Optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html).)\n",
    "\n",
    "Let's add a couple transformations to our DataFrame and look at the query plan on the resulting transformed DataFrame. Don't be too concerned if it looks like gibberish(胡言乱语；快速而不清楚的言语). As you gain more experience with Apache Spark, you'll begin to be able to use `explain()` to help you understand more about your DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- Aggregate [last_name#0, first_name#1, ssn#2, occupation#3, age#4L], [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "   +- LogicalRDD [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "last_name: string, first_name: string, ssn: string, occupation: string, age: bigint\n",
      "Project [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "+- Aggregate [last_name#0, first_name#1, ssn#2, occupation#3, age#4L], [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "   +- LogicalRDD [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [last_name#0, first_name#1, ssn#2, occupation#3, age#4L], [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "+- LogicalRDD [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "\n",
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[last_name#0, first_name#1, ssn#2, occupation#3, age#4L], functions=[], output=[last_name#0, first_name#1, ssn#2, occupation#3, age#4L])\n",
      "+- Exchange hashpartitioning(last_name#0, first_name#1, ssn#2, occupation#3, age#4L, 200)\n",
      "   +- *HashAggregate(keys=[last_name#0, first_name#1, ssn#2, occupation#3, age#4L], functions=[], output=[last_name#0, first_name#1, ssn#2, occupation#3, age#4L])\n",
      "      +- Scan ExistingRDD[last_name#0,first_name#1,ssn#2,occupation#3,age#4L]\n"
     ]
    }
   ],
   "source": [
    "newDF = dataDF.distinct().select('*')\n",
    "newDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3c): Subtract one from each value using _select_\n",
    "\n",
    "So far, we've created a distributed DataFrame that is split into many partitions, where each partition is stored on a single machine in our cluster.  Let's look at what happens when we do a basic operation on the dataset.  Many useful data analysis operations can be specified as \"do something to each item in the dataset\".  These data-parallel operations are convenient because each item in the dataset can be processed individually: the operation on one entry doesn't effect the operations on any of the other entries.  Therefore, Spark can parallelize the operation.\n",
    "\n",
    "One of the most common DataFrame operations is `select()`, and it works more or less like a SQL `SELECT` statement: You can select specific columns from the DataFrame, and you can even use `select()` to create _new_ columns with values that are derived from existing column values. We can use `select()` to create a new column that decrements the value of the existing `age` column.\n",
    "\n",
    "**`select()` is a _transformation_. It returns a new DataFrame that captures both the previous DataFrame and the operation to add to the query (`select`, in this case). But it does *not* actually execute anything on the cluster. When transforming DataFrames, we are building up a _query plan_. That query plan will be optimized, implemented (in terms of RDDs), and executed by Spark _only_ when we call an action.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform dataDF through a select transformation and rename the newly created '(age -1)' column to 'age'\n",
    "# Because select is a transformation and Spark uses lazy evaluation, no jobs, stages,\n",
    "# or tasks will be launched when we run this code.\n",
    "subDF = dataDF.select('last_name', 'first_name', 'ssn', 'occupation', (dataDF.age - 1).alias('age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the query plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('last_name, None), unresolvedalias('first_name, None), unresolvedalias('ssn, None), unresolvedalias('occupation, None), (age#4L - 1) AS age#19]\n",
      "+- LogicalRDD [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "last_name: string, first_name: string, ssn: string, occupation: string, age: bigint\n",
      "Project [last_name#0, first_name#1, ssn#2, occupation#3, (age#4L - cast(1 as bigint)) AS age#19L]\n",
      "+- LogicalRDD [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [last_name#0, first_name#1, ssn#2, occupation#3, (age#4L - 1) AS age#19L]\n",
      "+- LogicalRDD [last_name#0, first_name#1, ssn#2, occupation#3, age#4L]\n",
      "\n",
      "== Physical Plan ==\n",
      "*Project [last_name#0, first_name#1, ssn#2, occupation#3, (age#4L - 1) AS age#19L]\n",
      "+- Scan ExistingRDD[last_name#0,first_name#1,ssn#2,occupation#3,age#4L]\n"
     ]
    }
   ],
   "source": [
    "subDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way to visualize the data is to use the `show()` method. If you don't tell `show()` how many rows to display, it displays 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+--------------------+---+\n",
      "|last_name|first_name|        ssn|          occupation|age|\n",
      "+---------+----------+-----------+--------------------+---+\n",
      "|    Brown|     Jason|182-83-5988|Community educati...|  1|\n",
      "|    Brown|      Cody|298-53-9877|   Financial planner| 19|\n",
      "|  Griffin|    Sandra|175-58-0111|Community educati...| 10|\n",
      "|    Wyatt|     David|270-76-3455|Teaching laborato...| 24|\n",
      "|   George|    Daniel|200-38-3837| Medical illustrator| 18|\n",
      "|   Rogers|     Barry|634-25-3185|   Market researcher| 30|\n",
      "|   Foster|    Morgan|464-88-6116|Production assist...| 39|\n",
      "|   Hansen|      John|773-12-5058|Armed forces logi...| 42|\n",
      "|     Rice|     Derek|054-51-9007|Presenter, broadc...| 23|\n",
      "|   Little|    Cheryl|725-70-4549|Broadcast journalist| 41|\n",
      "|   Peters|      Leah|826-58-6908|   Therapist, sports|  8|\n",
      "|    Simon|     Logan|062-37-6157|Horticulturist, c...| 31|\n",
      "|  Bennett|   Krystal|308-43-0932|Engineer, communi...| 24|\n",
      "|    Burke|     Kelly|299-52-4282|Aeronautical engi...|  4|\n",
      "| Robinson|     Paige|212-42-9151|Communications en...|  7|\n",
      "|    Aaron|       Mr.|631-15-3272|  Charity fundraiser| 36|\n",
      "|   Carter|   Bradley|750-85-4885|   Building surveyor| 35|\n",
      "|    Lyons|     Chris|795-90-8059|Chartered loss ad...| 35|\n",
      "|  Gregory| Stephanie|800-68-1097|Editor, commissio...| 10|\n",
      "|    Blair|     Emily|528-14-5681|Chartered loss ad...|  2|\n",
      "+---------+----------+-----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3d) Use _collect_ to view results\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3d.png\" style=\"height:700px;float:right\"/>\n",
    "\n",
    "To see a list of elements decremented by one, we need to create a new list on the driver from the the data distributed in the executor nodes.  To do this we can call the `collect()` method on our DataFrame.  `collect()` is often used after transformations to **ensure that we are only returning a *small* amount of data to the driver.  This is done because the data returned to the driver must fit into the driver's available memory.  If not, the driver will crash**.\n",
    "\n",
    "The `collect()` method is the first action operation that we have encountered.  Action operations cause Spark to perform the (lazy) transformation operations that are required to compute the values returned by the action.  In our example, this means that tasks will now be launched to perform the **`createDataFrame`**,** `select`**, and **`collect`** operations.\n",
    "\n",
    "In the diagram, the dataset is broken into four partitions, so four `collect()` tasks are launched. Each task collects the entries in its partition and sends the result to the driver, which creates a list of the values, as shown in the figure below.\n",
    "\n",
    "Now let's run `collect()` on `subDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's collect the data\n",
    "results = subDF.collect()\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way to visualize the data is to use the `show()` method. If you don't tell `show()` how many rows to display, it displays 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+--------------------+---+\n",
      "|last_name|first_name|        ssn|          occupation|age|\n",
      "+---------+----------+-----------+--------------------+---+\n",
      "|    Brown|     Jason|182-83-5988|Community educati...|  1|\n",
      "|    Brown|      Cody|298-53-9877|   Financial planner| 19|\n",
      "|  Griffin|    Sandra|175-58-0111|Community educati...| 10|\n",
      "|    Wyatt|     David|270-76-3455|Teaching laborato...| 24|\n",
      "|   George|    Daniel|200-38-3837| Medical illustrator| 18|\n",
      "|   Rogers|     Barry|634-25-3185|   Market researcher| 30|\n",
      "|   Foster|    Morgan|464-88-6116|Production assist...| 39|\n",
      "|   Hansen|      John|773-12-5058|Armed forces logi...| 42|\n",
      "|     Rice|     Derek|054-51-9007|Presenter, broadc...| 23|\n",
      "|   Little|    Cheryl|725-70-4549|Broadcast journalist| 41|\n",
      "|   Peters|      Leah|826-58-6908|   Therapist, sports|  8|\n",
      "|    Simon|     Logan|062-37-6157|Horticulturist, c...| 31|\n",
      "|  Bennett|   Krystal|308-43-0932|Engineer, communi...| 24|\n",
      "|    Burke|     Kelly|299-52-4282|Aeronautical engi...|  4|\n",
      "| Robinson|     Paige|212-42-9151|Communications en...|  7|\n",
      "|    Aaron|       Mr.|631-15-3272|  Charity fundraiser| 36|\n",
      "|   Carter|   Bradley|750-85-4885|   Building surveyor| 35|\n",
      "|    Lyons|     Chris|795-90-8059|Chartered loss ad...| 35|\n",
      "|  Gregory| Stephanie|800-68-1097|Editor, commissio...| 10|\n",
      "|    Blair|     Emily|528-14-5681|Chartered loss ad...|  2|\n",
      "+---------+----------+-----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd prefer that `show()` not **`truncate`**(截断) the data, you can tell it not to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-----------------------------------------------------+---+\n",
      "|last_name |first_name|ssn        |occupation                                           |age|\n",
      "+----------+----------+-----------+-----------------------------------------------------+---+\n",
      "|Brown     |Jason     |182-83-5988|Community education officer                          |1  |\n",
      "|Brown     |Cody      |298-53-9877|Financial planner                                    |19 |\n",
      "|Griffin   |Sandra    |175-58-0111|Community education officer                          |10 |\n",
      "|Wyatt     |David     |270-76-3455|Teaching laboratory technician                       |24 |\n",
      "|George    |Daniel    |200-38-3837|Medical illustrator                                  |18 |\n",
      "|Rogers    |Barry     |634-25-3185|Market researcher                                    |30 |\n",
      "|Foster    |Morgan    |464-88-6116|Production assistant, radio                          |39 |\n",
      "|Hansen    |John      |773-12-5058|Armed forces logistics/support/administrative officer|42 |\n",
      "|Rice      |Derek     |054-51-9007|Presenter, broadcasting                              |23 |\n",
      "|Little    |Cheryl    |725-70-4549|Broadcast journalist                                 |41 |\n",
      "|Peters    |Leah      |826-58-6908|Therapist, sports                                    |8  |\n",
      "|Simon     |Logan     |062-37-6157|Horticulturist, commercial                           |31 |\n",
      "|Bennett   |Krystal   |308-43-0932|Engineer, communications                             |24 |\n",
      "|Burke     |Kelly     |299-52-4282|Aeronautical engineer                                |4  |\n",
      "|Robinson  |Paige     |212-42-9151|Communications engineer                              |7  |\n",
      "|Aaron     |Mr.       |631-15-3272|Charity fundraiser                                   |36 |\n",
      "|Carter    |Bradley   |750-85-4885|Building surveyor                                    |35 |\n",
      "|Lyons     |Chris     |795-90-8059|Chartered loss adjuster                              |35 |\n",
      "|Gregory   |Stephanie |800-68-1097|Editor, commissioning                                |10 |\n",
      "|Blair     |Emily     |528-14-5681|Chartered loss adjuster                              |2  |\n",
      "|Young     |Sherri    |366-97-6486|Multimedia specialist                                |45 |\n",
      "|Patterson |Bobby     |063-74-9852|Land                                                 |21 |\n",
      "|Weeks     |Sean      |414-34-0858|Forensic psychologist                                |45 |\n",
      "|Richardson|Heather   |046-51-7020|Drilling engineer                                    |20 |\n",
      "|Parsons   |Craig     |271-23-7752|Equality and diversity officer                       |35 |\n",
      "|Wallace   |Sabrina   |092-13-4391|Furniture conservator/restorer                       |34 |\n",
      "|Johnson   |Sherry    |665-96-8949|Psychologist, counselling                            |31 |\n",
      "|Fisher    |Steven    |898-49-3926|Advertising account planner                          |2  |\n",
      "|Hill      |Tanya     |644-05-1087|Runner, broadcasting/film/video                      |0  |\n",
      "|Singleton |Brittney  |296-98-1289|Counselling psychologist                             |6  |\n",
      "+----------+----------+-----------+-----------------------------------------------------+---+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subDF.show(n=30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3e) Use _count_ to get total\n",
    "\n",
    "One of the most basic jobs that we can run is the `count()` job which will count the number of elements in a DataFrame, using the `count()` action. Since `select()` creates a new DataFrame with the same number of elements as the starting DataFrame, we expect that applying `count()` to each DataFrame will return the same result.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3e.png\" style=\"height:700px;float:right\"/>\n",
    "\n",
    "Note that because `count()` is an action operation, if we had not already performed an action with `collect()`, then Spark would now perform the transformation operations when we executed `count()`.\n",
    "\n",
    "Each task counts the entries in its partition and sends the result to your SparkContext, which adds up all of the counts. The figure on the right shows what would happen if we ran `count()` on a small example dataset with just four partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(dataDF.count())\n",
    "print(subDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3f) Apply transformation _filter_ and view results with _collect_\n",
    "\n",
    "Next, we'll create a new DataFrame that only contains the people whose ages are less than 10. To do this, we'll use the **`filter()`** transformation. (You can also use `where()`, an alias for `filter()`, if you prefer something more SQL-like). The `filter()` method is a transformation operation that creates a new DataFrame from the input DataFrame, keeping only values that match the filter expression.\n",
    "\n",
    "The figure shows how this might work on the small four-partition dataset.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3f.png\" style=\"height:700px;float:right\"/>\n",
    "\n",
    "To view the filtered list of elements less than 10, we need to create a new list on the driver from the distributed data on the executor nodes.  We use the `collect()` method to return a list that contains all of the elements in this filtered DataFrame to the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+-------------------------------+---+\n",
      "|last_name|first_name |ssn        |occupation                     |age|\n",
      "+---------+-----------+-----------+-------------------------------+---+\n",
      "|Brown    |Jason      |182-83-5988|Community education officer    |1  |\n",
      "|Peters   |Leah       |826-58-6908|Therapist, sports              |8  |\n",
      "|Burke    |Kelly      |299-52-4282|Aeronautical engineer          |4  |\n",
      "|Robinson |Paige      |212-42-9151|Communications engineer        |7  |\n",
      "|Blair    |Emily      |528-14-5681|Chartered loss adjuster        |2  |\n",
      "|Fisher   |Steven     |898-49-3926|Advertising account planner    |2  |\n",
      "|Hill     |Tanya      |644-05-1087|Runner, broadcasting/film/video|0  |\n",
      "|Singleton|Brittney   |296-98-1289|Counselling psychologist       |6  |\n",
      "|Smith    |Brandon    |780-25-1953|Geoscientist                   |6  |\n",
      "|King     |Matthew    |263-65-2624|Lobbyist                       |7  |\n",
      "|Navarro  |William    |200-09-7607|Therapist, sports              |5  |\n",
      "|Williams |Ariel      |124-78-7492|Electronics engineer           |9  |\n",
      "|Ryan     |Christopher|385-53-9579|Lawyer                         |9  |\n",
      "|Johnson  |Megan      |028-72-8991|IT trainer                     |7  |\n",
      "|Hunt     |Logan      |134-48-1868|Data processing manager        |4  |\n",
      "|Clark    |Megan      |592-99-2891|Wellsite geologist             |7  |\n",
      "|Blevins  |Amy        |495-91-8267|Programmer, multimedia         |8  |\n",
      "|Robinson |Jessica    |247-36-1191|Hydrogeologist                 |5  |\n",
      "|Martinez |Patrick    |123-13-3579|Advertising account executive  |5  |\n",
      "|Morales  |Kendra     |773-23-3100|Newspaper journalist           |0  |\n",
      "+---------+-----------+-----------+-------------------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2043"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filteredDF = subDF.filter(subDF.age < 10)\n",
    "filteredDF.show(truncate=False)\n",
    "filteredDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(These are some seriously precocious children...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Python Lambda functions and User Defined Functions\n",
    "\n",
    "Python supports the use of small one-line **anonymous functions**(匿名函数)that are not bound to a name at runtime.\n",
    "\n",
    "`lambda` functions, borrowed from(借用了)LISP(全名LIStProcessor,即表处理语言), can be used wherever function objects are required. They are syntactically(在语法上)restricted to a single expression. Remember that `lambda` functions are **a matter of style** and using them is never required - semantically(语义上), they are just **syntactic sugar** for a normal function definition. You can always define a separate normal function instead, but using a `lambda` function is an equivalent and more compact(紧凑型,简洁的) form of coding. Ideally you should consider using `lambda` functions where you want to encapsulate(封装) non-reusable(不可重用的) code without littering(乱扔;使杂乱) your code with one-line functions.\n",
    "\n",
    "Here, instead of defining a separate function for the `filter()` transformation, we will use an inline `lambda()` function and we will register that lambda as a Spark <a target=\"_blank\" href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=udf#pyspark.sql.functions.udf\">_User Defined Function_ (UDF)</a>. A UDF is a special **wrapper** around a function, allowing the function to be used in a DataFrame query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+-------------------------------+---+\n",
      "|last_name|first_name |ssn        |occupation                     |age|\n",
      "+---------+-----------+-----------+-------------------------------+---+\n",
      "|Brown    |Jason      |182-83-5988|Community education officer    |1  |\n",
      "|Peters   |Leah       |826-58-6908|Therapist, sports              |8  |\n",
      "|Burke    |Kelly      |299-52-4282|Aeronautical engineer          |4  |\n",
      "|Robinson |Paige      |212-42-9151|Communications engineer        |7  |\n",
      "|Blair    |Emily      |528-14-5681|Chartered loss adjuster        |2  |\n",
      "|Fisher   |Steven     |898-49-3926|Advertising account planner    |2  |\n",
      "|Hill     |Tanya      |644-05-1087|Runner, broadcasting/film/video|0  |\n",
      "|Singleton|Brittney   |296-98-1289|Counselling psychologist       |6  |\n",
      "|Smith    |Brandon    |780-25-1953|Geoscientist                   |6  |\n",
      "|King     |Matthew    |263-65-2624|Lobbyist                       |7  |\n",
      "|Navarro  |William    |200-09-7607|Therapist, sports              |5  |\n",
      "|Williams |Ariel      |124-78-7492|Electronics engineer           |9  |\n",
      "|Ryan     |Christopher|385-53-9579|Lawyer                         |9  |\n",
      "|Johnson  |Megan      |028-72-8991|IT trainer                     |7  |\n",
      "|Hunt     |Logan      |134-48-1868|Data processing manager        |4  |\n",
      "|Clark    |Megan      |592-99-2891|Wellsite geologist             |7  |\n",
      "|Blevins  |Amy        |495-91-8267|Programmer, multimedia         |8  |\n",
      "|Robinson |Jessica    |247-36-1191|Hydrogeologist                 |5  |\n",
      "|Martinez |Patrick    |123-13-3579|Advertising account executive  |5  |\n",
      "|Morales  |Kendra     |773-23-3100|Newspaper journalist           |0  |\n",
      "+---------+-----------+-----------+-------------------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2043"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "less_ten = udf(lambda s: s < 10, BooleanType())\n",
    "lambdaDF = subDF.filter(less_ten(subDF.age))\n",
    "lambdaDF.show(truncate=False)\n",
    "lambdaDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Additional DataFrame actions\n",
    "\n",
    "Let's investigate(研究) some additional actions:\n",
    "\n",
    "* [first()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.first)\n",
    "* [take()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.take)\n",
    "\n",
    "One useful thing to do when we have a new dataset is to look at the first few entries to obtain a rough idea of what information is available.  In Spark, we can do that using actions like **`first()`**, **`take()`**, and **`show()`**. Note that for the `first()` and `take()` actions, the elements that are returned depend on how the DataFrame is *partitioned*.\n",
    "\n",
    "Instead of using the `collect()` action, we can use the `take(n)` action to return the first _n_ elements of the DataFrame. The `first()` action returns the first element of a DataFrame, and is equivalent to `take(1)[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first:Row(last_name='Brown', first_name='Jason', ssn='182-83-5988', occupation='Community education officer', age=1)\n",
      "\n",
      "Four of them:[Row(last_name='Brown', first_name='Jason', ssn='182-83-5988', occupation='Community education officer', age=1), Row(last_name='Peters', first_name='Leah', ssn='826-58-6908', occupation='Therapist, sports', age=8), Row(last_name='Burke', first_name='Kelly', ssn='299-52-4282', occupation='Aeronautical engineer', age=4), Row(last_name='Robinson', first_name='Paige', ssn='212-42-9151', occupation='Communications engineer', age=7)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"first:{0}\\n\".format(filteredDF.first()))\n",
    "print(\"Four of them:{0}\\n\".format(filteredDF.take(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Additional DataFrame transformations\n",
    "### (6a) _orderBy_\n",
    "\n",
    "[`orderBy()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct) allows you to sort a DataFrame by one or more columns, producing a new DataFrame.\n",
    "\n",
    "For example, let's get the first five oldest people in the original (unfiltered) DataFrame. We can use the `orderBy()` transformation. `orderBy` takes one or more columns, either as _names_ (strings) or as `Column` objects. To get a `Column` object, we use one of two notations(标记法) on the DataFrame:\n",
    "\n",
    "* Pandas-style notation: `filteredDF.age`\n",
    "* Subscript notation: `filteredDF['age']`\n",
    "\n",
    "Both of those syntaxes return a `Column`, which has additional methods like `desc()` (for sorting in descending order) or `asc()` (for sorting in ascending order, which is the default).\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "```\n",
    "dataDF.orderBy(dataDF['age'])  # sort by age in ascending order(升序排序); returns a new DataFrame\n",
    "dataDF.orderBy(dataDF.last_name.desc()) # sort by last name in descending order(降序排序)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(last_name='Rodriguez', first_name='Jared', ssn='466-45-7301', occupation='Network engineer', age=47),\n",
       " Row(last_name='Jones', first_name='Mary', ssn='319-27-2359', occupation='Midwife', age=47),\n",
       " Row(last_name='Simon', first_name='Carolyn', ssn='080-30-7534', occupation='Hydrologist', age=47),\n",
       " Row(last_name='Smith', first_name='Edward', ssn='051-21-6125', occupation='Museum education officer', age=47),\n",
       " Row(last_name='Williams', first_name='Christine', ssn='101-44-4714', occupation='Engineer, technical sales', age=47)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDF.orderBy(dataDF.age.desc()).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reverse the sort order(颠倒排序顺序). Since ascending sort is the default, we can actually use a `Column` object expression or a simple string, in this case. The `desc()` and `asc()` methods are only defined on `Column`. Something like `orderBy('age'.desc())` would not work, because there's no `desc()` method on Python string objects. That's why we needed the column expression. But if we're just using the defaults, we can pass a string column name into `orderBy()`. This is sometimes easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(last_name='Hill', first_name='Tanya', ssn='644-05-1087', occupation='Runner, broadcasting/film/video', age=1),\n",
       " Row(last_name='Morales', first_name='Kendra', ssn='773-23-3100', occupation='Newspaper journalist', age=1),\n",
       " Row(last_name='Mcintyre', first_name='Sandra', ssn='276-99-8258', occupation='Technical author', age=1),\n",
       " Row(last_name='Lawson', first_name='Julian', ssn='569-44-8775', occupation='Public librarian', age=1),\n",
       " Row(last_name='Wilkins', first_name='Kelsey', ssn='884-06-6723', occupation='Trade union research officer', age=1)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDF.orderBy('age').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6b) _distinct_ and _dropDuplicates_\n",
    "\n",
    "[`distinct()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct) filters out duplicate(完全一样的) rows, and it considers all columns. Since our data is completely randomly generated (by `fake-factory`), it's extremely unlikely(完全不可能) that there are any duplicate rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(dataDF.count())\n",
    "print(dataDF.distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate `distinct()`, let's create a quick throwaway dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tempDF = sqlContext.createDataFrame([(\"Joe\", 1), (\"Joe\", 1), (\"Anna\", 15), (\"Anna\", 12), (\"Ravi\", 5)], ('name', 'score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|score|\n",
      "+----+-----+\n",
      "| Joe|    1|\n",
      "| Joe|    1|\n",
      "|Anna|   15|\n",
      "|Anna|   12|\n",
      "|Ravi|    5|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|score|\n",
      "+----+-----+\n",
      "| Joe|    1|\n",
      "|Ravi|    5|\n",
      "|Anna|   12|\n",
      "|Anna|   15|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that one of the (\"Joe\", 1) rows was deleted, but both rows with name \"Anna\" were kept, **because all columns in a row must match another row for it to be considered a duplicate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6c) _drop_\n",
    "\n",
    "[`drop()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.drop) is like the opposite of `select()`: Instead of selecting specific columns from a DataFrame, **it drops a specifed column from a DataFrame**.\n",
    "\n",
    "Here's a simple use case: Suppose you're reading from a 1,000-column CSV file, and you have to get rid of(扔掉) five of the columns. **Instead of selecting 995 of the columns, it's easier just to drop the five you don't want**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|last_name|first_name|        ssn|\n",
      "+---------+----------+-----------+\n",
      "|    Brown|     Jason|182-83-5988|\n",
      "|    Brown|      Cody|298-53-9877|\n",
      "|  Griffin|    Sandra|175-58-0111|\n",
      "|    Wyatt|     David|270-76-3455|\n",
      "|   George|    Daniel|200-38-3837|\n",
      "|   Rogers|     Barry|634-25-3185|\n",
      "|   Foster|    Morgan|464-88-6116|\n",
      "|   Hansen|      John|773-12-5058|\n",
      "|     Rice|     Derek|054-51-9007|\n",
      "|   Little|    Cheryl|725-70-4549|\n",
      "|   Peters|      Leah|826-58-6908|\n",
      "|    Simon|     Logan|062-37-6157|\n",
      "|  Bennett|   Krystal|308-43-0932|\n",
      "|    Burke|     Kelly|299-52-4282|\n",
      "| Robinson|     Paige|212-42-9151|\n",
      "|    Aaron|       Mr.|631-15-3272|\n",
      "|   Carter|   Bradley|750-85-4885|\n",
      "|    Lyons|     Chris|795-90-8059|\n",
      "|  Gregory| Stephanie|800-68-1097|\n",
      "|    Blair|     Emily|528-14-5681|\n",
      "+---------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.drop('occupation').drop('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6d) _groupBy_\n",
    "\n",
    "[groupBy()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) is one of the most powerful transformations. **It allows you to perform aggregations(聚合,集合,汇总) on a DataFrame**.\n",
    "\n",
    "Unlike other DataFrame transformations, `groupBy()` does _not_ return a DataFrame. Instead, it returns a special [GroupedData](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData) object that contains various aggregation functions.\n",
    "\n",
    "The most commonly used aggregation function is [count()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.count),\n",
    "but there are others (like [sum()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.sum), [max()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.max), and [avg()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.avg).\n",
    "\n",
    "These **aggregation functions** typically create a new column and return a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-----+\n",
      "|occupation                             |count|\n",
      "+---------------------------------------+-----+\n",
      "|Librarian, academic                    |15   |\n",
      "|Engineer, aeronautical                 |16   |\n",
      "|Retail merchandiser                    |18   |\n",
      "|Catering manager                       |23   |\n",
      "|Diplomatic Services operational officer|13   |\n",
      "|Designer, ceramics/pottery             |14   |\n",
      "|Patent examiner                        |16   |\n",
      "|Occupational hygienist                 |10   |\n",
      "|Early years teacher                    |19   |\n",
      "|Primary school teacher                 |16   |\n",
      "|English as a second language teacher   |12   |\n",
      "|Estate agent                           |22   |\n",
      "|Clinical molecular geneticist          |16   |\n",
      "|Control and instrumentation engineer   |12   |\n",
      "|Applications developer                 |12   |\n",
      "|Art therapist                          |21   |\n",
      "|Transport planner                      |16   |\n",
      "|Conservator, furniture                 |15   |\n",
      "|Petroleum engineer                     |12   |\n",
      "|Stage manager                          |10   |\n",
      "+---------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.groupBy('occupation').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|age|avg(age)|\n",
      "+---+--------+\n",
      "|29 |29.0    |\n",
      "|26 |26.0    |\n",
      "|19 |19.0    |\n",
      "|22 |22.0    |\n",
      "|7  |7.0     |\n",
      "|34 |34.0    |\n",
      "|43 |43.0    |\n",
      "|32 |32.0    |\n",
      "|31 |31.0    |\n",
      "|39 |39.0    |\n",
      "|25 |25.0    |\n",
      "|6  |6.0     |\n",
      "|9  |9.0     |\n",
      "|27 |27.0    |\n",
      "|17 |17.0    |\n",
      "|41 |41.0    |\n",
      "|33 |33.0    |\n",
      "|28 |28.0    |\n",
      "|5  |5.0     |\n",
      "|1  |1.0     |\n",
      "+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.groupBy(dataDF.age).avg().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|24.1735 |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.groupBy().avg(\"age\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `groupBy()` to do aother useful aggregations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum age:47\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum age:{0}\".format(dataDF.groupBy().max('age').first()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum age: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum age: {0}\".format(dataDF.groupBy().min('age').first()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6e) _sample_ (optional)\n",
    "\n",
    "When analyzing data, the [`sample()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sample) transformation is often quite useful. It returns a new DataFrame with a random sample of elements from the dataset.  It takes in a `withReplacement` argument, which specifies whether it is okay to randomly pick the same item multiple times from the parent DataFrame (so when `withReplacement=True`, you can get the same item back multiple times). It takes in a `fraction` parameter, which specifies the fraction elements in the dataset you want to return. (So a `fraction` value of `0.20` returns 20% of the elements in the DataFrame.) It also takes an optional `seed` parameter that allows you to specify a seed value for the random number generator, so that reproducible(可再现的) results can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "939\n",
      "+---------+----------+-----------+--------------------+---+\n",
      "|last_name|first_name|        ssn|          occupation|age|\n",
      "+---------+----------+-----------+--------------------+---+\n",
      "|    Brown|      Cody|298-53-9877|   Financial planner| 20|\n",
      "|   Hansen|      John|773-12-5058|Armed forces logi...| 43|\n",
      "|   Peters|      Leah|826-58-6908|   Therapist, sports|  9|\n",
      "| Robinson|     Paige|212-42-9151|Communications en...|  8|\n",
      "|  Gregory| Stephanie|800-68-1097|Editor, commissio...| 11|\n",
      "|  Johnson|    Sherry|665-96-8949|Psychologist, cou...| 32|\n",
      "|   Fisher|    Steven|898-49-3926|Advertising accou...|  3|\n",
      "|  Navarro|   William|200-09-7607|   Therapist, sports|  6|\n",
      "|   Mathis|       Ana|602-42-1858|Horticulturist, a...| 30|\n",
      "|   George|   Raymond|004-76-6251|         Illustrator| 39|\n",
      "|   Garcia|  Danielle|717-05-1050|   Financial adviser| 11|\n",
      "|     Hall|    Joseph|712-61-2407|  Purchasing manager| 46|\n",
      "|     Mays|     Edgar|128-90-3792|    Heritage manager| 26|\n",
      "|    Davis|   Shannon|517-20-4894|    Sports therapist| 14|\n",
      "|    Gomez|     Sheri|436-05-6057|Research scientis...|  2|\n",
      "|     Chen|   Anthony|203-32-2098|Engineer, electro...| 17|\n",
      "|    Simon|   Carolyn|080-30-7534|         Hydrologist| 47|\n",
      "|    Olsen|    Curtis|647-48-6353|Commercial/reside...| 37|\n",
      "|   Steele|     Megan|191-99-1465|   Financial planner| 28|\n",
      "|    Greer|     Amber|403-01-7565|Intelligence analyst| 10|\n",
      "+---------+----------+-----------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataDF.count())\n",
    "sampledDF = dataDF.sample(withReplacement=False, fraction=0.10)\n",
    "print(sampledDF.count())\n",
    "sampledDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485\n"
     ]
    }
   ],
   "source": [
    "print(dataDF.sample(withReplacement=True, fraction=0.05).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Caching DataFrames and storage options\n",
    "### (7a) Caching DataFrames\n",
    "\n",
    "For efficiency Spark keeps your DataFrames in memory. (More formally, it keeps the _RDDs_ that implement your DataFrames in memory.) By keeping the contents in memory, Spark can quickly access the data. However, memory is limited**(内存有限)**, so if you try to keep too many partitions in memory, Spark will automatically delete partitions from memory to make space for new ones. If you later refer to one of the deleted partitions, Spark will automatically **recreate** it for you, but that takes time.\n",
    "\n",
    "So, if you plan to use a DataFrame more than once, then you should tell Spark to cache it. You can use the `cache()` operation to keep the DataFrame in memory. However, you must still trigger an action on the DataFrame, such as `collect()` or `count()` before the caching will occur. In other words, **`cache()` is lazy**: It merely**(仅仅;只不过)** tells Spark that the DataFrame should be cached _when the data is **materialized**_. You have to run an action to materialize the data; the DataFrame will be cached as a side effect. The next time you use the DataFrame, Spark will use the cached data, rather than recomputing the DataFrame from the original data.\n",
    "\n",
    "You can see your cached DataFrame in the \"**Storage**\" section of the **Spark web UI**. If you click on the name value, you can see more information about where the the DataFrame is stored.\n",
    "\n",
    "by zhaolianrui：Check if it is cached you can ues the method ：**is_cached**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2043\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Cache the DataFrame\n",
    "filteredDF.cache()\n",
    "# Trigger an action\n",
    "print(filteredDF.count())\n",
    "# Check if it is cached\n",
    "print(filteredDF.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7b) Unpersist and storage options\n",
    "\n",
    "Spark automatically manages the partitions cached in memory. If it has more partitions than available memory, by default, it will evict(删除) older partitions to make room for new ones. For efficiency, once you are finished using cached DataFrame, you can optionally tell Spark to stop caching it in memory by using the DataFrame's `**unpersist()**` method to inform Spark that you no longer need the cached data.\n",
    "\n",
    "** Advanced: ** Spark provides many more options for managing how DataFrames cached. For instance, you can tell Spark to spill cached partitions to disk when it runs out of memory, instead of simply throwing old ones away. You can explore the API for DataFrame's [persist()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.persist) operation using Python's [help()](https://docs.python.org/2/library/functions.html?highlight=help#help) command.  The `persist()` operation, optionally, takes a pySpark [StorageLevel](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.StorageLevel) object.\n",
    "\n",
    "**by:zhaolianrui**\n",
    "\n",
    "1）RDD的**cache()**方法其实调用的就是**persist**方法，缓存策略均为**MEMORY_ONLY**;\n",
    "\n",
    "2）可以通过**persist**方法手工设定**StorageLevel**来满足工程需要的存储级别;\n",
    "\n",
    "3）**cache**或者**persist**并不是action;\n",
    "\n",
    "**cache**和**persist**都可以用**unpersist**来取消"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# If we are done with the DataFrame we can unpersist it so that its memory can be reclaimed\n",
    "filteredDF.unpersist()\n",
    "# Check if it is cached\n",
    "print(filteredDF.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Part 8: Debugging Spark applications and lazy evaluation **\n",
    "### How Python is Executed in Spark\n",
    "\n",
    "Internally(在内部), Spark executes using a Java Virtual Machine (JVM). pySpark runs Python code in a JVM using [Py4J](http://py4j.sourceforge.net). Py4J enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. Methods are called as if the Java objects resided(存在，驻留) in the Python interpreter and Java collections can be accessed through standard Python collection methods. Py4J also enables Java programs to call back Python objects.\n",
    "\n",
    "Because pySpark uses Py4J, coding errors often result in a complicated, confusing stack trace that can be difficult to understand. In the following section, we'll explore how to understand **stack traces**.\n",
    "### (8a) Challenges with lazy evaluation using transformations and actions\n",
    "\n",
    "Spark's use of `lazy evaluation` can make debugging more difficult because code is not always executed immediately. To see an example of how this can happen, let's first define a broken filter function.\n",
    "Next we perform a `filter()` operation using the broken filtering function.  No error will occur at this point due to Spark's use of lazy evaluation.\n",
    "\n",
    "The `filter()` method will not be executed *until* an action operation is invoked on the DataFrame.  We will perform an action by using the `count()` method to return a list that contains all of the elements in this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def brokenTen(value):\n",
    "    \"\"\"Incorrect implementation of the ten function.\n",
    "\n",
    "    Note:\n",
    "        The `if` statement checks an undefined variable `val` instead of `value`.\n",
    "\n",
    "    Args:\n",
    "        value (int): A number.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether `value` is less than ten.\n",
    "\n",
    "    Raises:\n",
    "        NameError: The function references `val`, which is not available in the local or global\n",
    "            namespace, so a `NameError` is raised.\n",
    "    \"\"\"\n",
    "    if (val < 10):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "btUDF = udf(brokenTen)\n",
    "brokenDF = subDF.filter(btUDF(subDF.age) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we'll see the error\n",
    "# Click on the `+` button to expand the error and scroll through the message.\n",
    "#brokenDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8b) Finding the bug\n",
    "\n",
    "When the `filter()` method is executed, Spark calls the UDF. Since our UDF has an error in the underlying filtering function `brokenTen()`, an error occurs.\n",
    "\n",
    "Scroll through the output \"Py4JJavaError     Traceback (most recent call last)\" part of the cell and first you will see that the line that generated the error is the `count()` method line. There is *nothing wrong with this line*. However, it is an action and that caused other methods to be executed. Continue scrolling through the Traceback and you will see the following error line:\n",
    "\n",
    "`NameError: global name 'val' is not defined`\n",
    "\n",
    "Looking at this error line, we can see that we used the wrong variable name in our filtering function `brokenTen()`.\n",
    "### (8c) Moving toward expert style\n",
    "\n",
    "As you are learning Spark, I recommend that you write your code in the form:\n",
    "```\n",
    "    df2 = df1.transformation1()\n",
    "    df2.action1()\n",
    "    df3 = df2.transformation2()\n",
    "    df3.action2()\n",
    "```\n",
    "Using this style will make debugging your code much easier as it makes errors easier to localize - errors in your transformations will occur when the next action is executed.\n",
    "\n",
    "Once you become more experienced with Spark, you can write your code with the form: `df.transformation1().transformation2().action()`\n",
    "\n",
    "We can also use `lambda()` functions instead of separately(单独的) defined functions when their use improves readability and conciseness(可读性和简洁性)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[last_name: string, first_name: string, ssn: string, occupation: string, age: bigint]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaner code through lambda use\n",
    "myUDF = udf(lambda v: v < 10)\n",
    "subDF.filter(myUDF(subDF.age) == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8d) Readability and code style\n",
    "\n",
    "To make the expert coding style more readable, **enclose** the statement in parentheses(**小括号**) and put each method, transformation, or action on a separate line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----------------------------------------------------+\n",
      "|concat(first_name,  , last_name)|occupation                                           |\n",
      "+--------------------------------+-----------------------------------------------------+\n",
      "|David Wyatt                     |Teaching laboratory technician                       |\n",
      "|Barry Rogers                    |Market researcher                                    |\n",
      "|Morgan Foster                   |Production assistant, radio                          |\n",
      "|John Hansen                     |Armed forces logistics/support/administrative officer|\n",
      "|Derek Rice                      |Presenter, broadcasting                              |\n",
      "|Cheryl Little                   |Broadcast journalist                                 |\n",
      "|Logan Simon                     |Horticulturist, commercial                           |\n",
      "|Krystal Bennett                 |Engineer, communications                             |\n",
      "|Mr. Aaron                       |Charity fundraiser                                   |\n",
      "|Bradley Carter                  |Building surveyor                                    |\n",
      "|Chris Lyons                     |Chartered loss adjuster                              |\n",
      "|Sherri Young                    |Multimedia specialist                                |\n",
      "|Bobby Patterson                 |Land                                                 |\n",
      "|Sean Weeks                      |Forensic psychologist                                |\n",
      "|Heather Richardson              |Drilling engineer                                    |\n",
      "|Craig Parsons                   |Equality and diversity officer                       |\n",
      "|Sabrina Wallace                 |Furniture conservator/restorer                       |\n",
      "|Sherry Johnson                  |Psychologist, counselling                            |\n",
      "|Joyce Bishop                    |Intelligence analyst                                 |\n",
      "|Dr. Robert                      |Therapist, art                                       |\n",
      "+--------------------------------+-----------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final version\n",
    "from pyspark.sql.functions import *\n",
    "(dataDF\n",
    " .filter(dataDF.age > 20)\n",
    " .select(concat(dataDF.first_name, lit(' '), dataDF.last_name), dataDF.occupation)\n",
    " .show(truncate=False)\n",
    " )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
